plan:
  # Continuation of adaptive learning system implementation
  # This file contains tasks 13-25 plus testing strategy and resources
  metadata:
    feature_name: "Adaptive Learning System for Conductor v2.0.0 (Part 2)"
    created: "2025-01-12"
    file_sequence: 2
    total_files: 2
    previous_file: "plan-01-foundation.yaml"
    tasks_in_file: [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]
  # Tasks continued from plan-01
  tasks:
    - task_number: 13
      name: "Update configuration schema with learning settings"
      agent: "golang-pro"
      worktree_group: "independent-13-config"
      files:
        - "internal/config/config.go"
        - "internal/config/config_test.go"
        - ".conductor/config.yaml.example"
      depends_on: []
      estimated_time: "45m"
      description: |
        Add learning configuration fields to the config schema including enable/disable flags,
        database path, behavior settings, and database maintenance options.
      test_first:
        test_file: "internal/config/config_test.go"
        structure:
          - "TestConfig_LearningDefaults"
          - "TestConfig_LearningDisabled"
          - "TestConfig_LearningCustomPath"
        mocks: []
        fixtures:
          - "Sample config YAML with learning section"
        assertions:
          - "Learning enabled by default"
          - "Default values correct"
          - "Custom values override defaults"
        edge_cases:
          - "Learning section missing (use defaults)"
          - "Invalid path specified"
        example_skeleton: |
          func TestConfig_LearningDefaults(t *testing.T) {
              config := &Config{}
              setDefaults(config)

              assert.True(t, config.Learning.Enabled)
              assert.Equal(t, ".conductor/learning", config.Learning.DBPath)
              assert.Equal(t, 2, config.Learning.MinFailuresBeforeAdapt)
          }
      implementation:
        approach: |
          1. Add LearningConfig struct to config.go
          2. Include in main Config struct
          3. Set defaults in setDefaults function
          4. Update config merging logic
          5. Document all fields
        code_structure: |
          type LearningConfig struct {
              Enabled                bool   `yaml:"enabled"`
              DBPath                 string `yaml:"db_path"`
              AutoAdaptAgent         bool   `yaml:"auto_adapt_agent"`
              EnhancePrompts         bool   `yaml:"enhance_prompts"`
              MinFailuresBeforeAdapt int    `yaml:"min_failures_before_adapt"`
              KeepExecutionsDays     int    `yaml:"keep_executions_days"`
              MaxExecutionsPerTask   int    `yaml:"max_executions_per_task"`
          }

          type Config struct {
              // existing fields
              Learning LearningConfig `yaml:"learning"`
          }
        key_points:
          - point: "Default values"
            details: "enabled: true, db_path: .conductor/learning, min_failures: 2"
          - point: "Backward compatibility"
            details: "Missing learning section uses defaults"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "Validate paths are not empty"
            - "Validate numeric values are positive"
      verification:
        manual_testing:
          - step: "Run config tests"
            command: "go test ./internal/config/ -v"
            expected: "All config tests pass"
        automated_tests:
          command: "go test ./internal/config/ -v"
          expected_output: "PASS"
        success_criteria:
          - "Config loads with learning section"
          - "Defaults work correctly"
      commit:
        type: "feat"
        message: "add learning configuration schema"
        body: |
          Add learning settings to config:
          - LearningConfig struct with all settings
          - Default values (enabled: true)
          - Config merging support
          - Full test coverage
          - Update example config
        files:
          - "internal/config/config.go"
          - "internal/config/config_test.go"
          - ".conductor/config.yaml.example"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 14
      name: "Implement session ID generation with timestamp format"
      agent: "golang-pro"
      worktree_group: "chain-4-session-management"
      files:
        - "internal/executor/session.go"
        - "internal/executor/session_test.go"
      depends_on: []
      estimated_time: "30m"
      description: |
        Create session management utilities for generating unique session IDs
        in the format session-YYYYMMDD-HHMM and handling session-related logic.
      test_first:
        test_file: "internal/executor/session_test.go"
        structure:
          - "TestGenerateSessionID_Format"
          - "TestGenerateSessionID_Unique"
        mocks: []
        fixtures: []
        assertions:
          - "Session ID matches format"
          - "IDs are unique across calls"
        edge_cases: []
        example_skeleton: |
          func TestGenerateSessionID_Format(t *testing.T) {
              sessionID := generateSessionID()

              assert.Regexp(t, `^session-\d{8}-\d{4}$`, sessionID)
          }
      implementation:
        approach: |
          1. Create session.go in executor package
          2. Implement generateSessionID using time.Now()
          3. Format as session-YYYYMMDD-HHMM
          4. Export for use by orchestrator
        code_structure: |
          func generateSessionID() string {
              now := time.Now()
              return fmt.Sprintf("session-%s", now.Format("20060102-1504"))
          }
        key_points:
          - point: "Time format"
            details: "Use Go time format: 20060102-1504"
        integration:
          imports:
            - "import \"time\""
            - "import \"fmt\""
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Run session tests"
            command: "go test ./internal/executor/session_test.go -v"
            expected: "Format tests pass"
        automated_tests:
          command: "go test ./internal/executor/ -run Session -v"
          expected_output: "PASS"
        success_criteria:
          - "Session IDs generated correctly"
          - "Format is consistent"
      commit:
        type: "feat"
        message: "add session ID generation and management"
        body: |
          Implement session management:
          - generateSessionID with timestamp format
          - Format: session-YYYYMMDD-HHMM
          - Test coverage for format validation
        files:
          - "internal/executor/session.go"
          - "internal/executor/session_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 15
      name: "Implement run number tracking per plan file"
      agent: "golang-pro"
      worktree_group: "chain-4-session-management"
      files:
        - "internal/learning/store.go"
        - "internal/learning/store_test.go"
      depends_on: [2, 14]
      estimated_time: "30m"
      description: |
        Add GetRunCount method to learning store to track how many times
        a plan file has been executed, enabling run number calculation.
      test_first:
        test_file: "internal/learning/store_test.go"
        structure:
          - "TestGetRunCount_NoExecutions"
          - "TestGetRunCount_MultipleRuns"
        mocks: []
        fixtures: []
        assertions:
          - "Returns 0 for new plans"
          - "Returns correct count for executed plans"
        edge_cases:
          - "Plan never executed"
          - "Multiple executions same run"
        example_skeleton: |
          func TestGetRunCount_MultipleRuns(t *testing.T) {
              store := setupTestStore(t)

              // Simulate 3 runs
              recordExecutionsForRuns(store, "plan.md", 3)

              count := store.GetRunCount("plan.md")
              assert.Equal(t, 3, count)
          }
      implementation:
        approach: |
          1. Add GetRunCount method to Store
          2. Query MAX(run_number) for plan file
          3. Return 0 if no executions found
          4. Use in orchestrator for run number calculation
        code_structure: |
          func (s *Store) GetRunCount(planFile string) int {
              var maxRun int
              query := `SELECT COALESCE(MAX(run_number), 0) FROM task_executions WHERE plan_file = ?`
              s.db.QueryRow(query, planFile).Scan(&maxRun)
              return maxRun
          }
        key_points:
          - point: "COALESCE for NULL handling"
            details: "Returns 0 if no rows found"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return 0 on error"
      verification:
        manual_testing:
          - step: "Run store tests"
            command: "go test ./internal/learning/ -run RunCount -v"
            expected: "Run counting works"
        automated_tests:
          command: "go test ./internal/learning/ -v"
          expected_output: "PASS"
        success_criteria:
          - "Correct run counts returned"
          - "Handles missing data gracefully"
      commit:
        type: "feat"
        message: "add run number tracking per plan file"
        body: |
          Implement run count tracking:
          - GetRunCount method in store
          - Query MAX(run_number) per plan
          - Handle NULL with COALESCE
          - Test coverage
        files:
          - "internal/learning/store.go"
          - "internal/learning/store_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 16
      name: "Update .gitignore to exclude learning database"
      agent: "fullstack-developer"
      worktree_group: "independent-16-gitignore"
      files:
        - ".gitignore"
      depends_on: []
      estimated_time: "5m"
      description: |
        Add .conductor/learning/ directory to .gitignore to prevent accidentally
        committing project-local learning databases to version control.
      test_first:
        test_file: "N/A - simple file edit"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Open .gitignore
          2. Add .conductor/learning/ entry
          3. Add comment explaining purpose
        code_structure: |
          # Conductor learning database (project-local)
          .conductor/learning/
        key_points:
          - point: "Directory path"
            details: "Use trailing slash to ignore directory"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Check gitignore"
            command: "git check-ignore .conductor/learning/test.db"
            expected: "Path is ignored"
        automated_tests:
          command: "N/A"
          expected_output: "N/A"
        success_criteria:
          - "Learning directory ignored"
          - "Pattern works correctly"
      commit:
        type: "chore"
        message: "add learning directory to gitignore"
        body: |
          Exclude learning database from git:
          - Add .conductor/learning/ to .gitignore
          - Prevent accidental commits of learning data
        files:
          - ".gitignore"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 17
      name: "Add mattn/go-sqlite3 dependency to go.mod"
      agent: "golang-pro"
      worktree_group: "independent-17-dependencies"
      files:
        - "go.mod"
        - "go.sum"
      depends_on: []
      estimated_time: "5m"
      description: |
        Add github.com/mattn/go-sqlite3 to project dependencies for SQLite database support.
      test_first:
        test_file: "N/A - dependency management"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Run go get github.com/mattn/go-sqlite3
          2. Verify go.mod and go.sum updated
          3. Ensure CGO is available for build
        code_structure: |
          # Command to run:
          go get github.com/mattn/go-sqlite3
        key_points:
          - point: "CGO requirement"
            details: "mattn/go-sqlite3 requires CGO compiler"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Add dependency"
            command: "go get github.com/mattn/go-sqlite3"
            expected: "Dependency added"
          - step: "Build project"
            command: "go build ./cmd/conductor"
            expected: "Builds successfully"
        automated_tests:
          command: "go mod verify"
          expected_output: "all modules verified"
        success_criteria:
          - "Dependency added to go.mod"
          - "Project builds successfully"
      commit:
        type: "chore"
        message: "add mattn/go-sqlite3 dependency"
        body: |
          Add SQLite database support:
          - Add github.com/mattn/go-sqlite3
          - Update go.mod and go.sum
          - Required for learning system
        files:
          - "go.mod"
          - "go.sum"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 18
      name: "Create integration tests for full learning pipeline"
      agent: "test-automator"
      worktree_group: "chain-5-integration-tests"
      files:
        - "internal/executor/learning_integration_test.go"
      depends_on: [15]
      estimated_time: "2h"
      description: |
        Build comprehensive integration tests that validate the complete learning pipeline
        from task execution through recording to analysis and adaptation.
      test_first:
        test_file: "internal/executor/learning_integration_test.go"
        structure:
          - "TestLearningPipeline_SingleTask"
          - "TestLearningPipeline_FailureAdaptation"
          - "TestLearningPipeline_PatternDetection"
          - "TestLearningPipeline_RunNumberTracking"
        mocks: []
        fixtures:
          - "Test plan files"
          - "In-memory database"
        assertions:
          - "Full pipeline executes end-to-end"
          - "Failures trigger adaptation"
          - "Patterns detected and stored"
          - "Run numbers increment correctly"
        edge_cases:
          - "Learning store unavailable"
          - "First run vs subsequent runs"
        example_skeleton: |
          func TestLearningPipeline_FailureAdaptation(t *testing.T) {
              // Setup: Create plan, learning store
              store := learning.NewStore(":memory:")
              plan := createTestPlan()

              // Simulate 2 failures with backend-developer
              recordFailures(store, "plan.md", "Task 1", "backend-developer", 2)

              // Execute with learning
              orchestrator := NewOrchestrator(plan, Config{
                  LearningStore: store,
              })

              result := orchestrator.Execute(context.Background())

              // Verify agent was switched
              assert.NotEqual(t, "backend-developer", result.AgentUsed)
          }
      implementation:
        approach: |
          1. Create integration test file
          2. Setup in-memory SQLite database
          3. Test complete execution flow with learning
          4. Verify all hooks execute correctly
          5. Test cross-run learning scenarios
          6. Validate database state after execution
        code_structure: |
          // Full pipeline test structure
          func TestLearningPipeline_XXX(t *testing.T) {
              // 1. Setup
              store := setupTestStore()
              plan := createTestPlan()

              // 2. Execute
              orch := NewOrchestrator(plan, Config{...})
              result := orch.Execute(ctx)

              // 3. Verify
              assertLearningBehavior(t, store, result)

              // 4. Cleanup
              store.Close()
          }
        key_points:
          - point: "In-memory database"
            details: "Use :memory: for fast isolated tests"
          - point: "End-to-end validation"
            details: "Test full flow not just units"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "Each test cleans up resources"
      verification:
        manual_testing:
          - step: "Run integration tests"
            command: "go test ./internal/executor/ -run Integration -v"
            expected: "All integration tests pass"
        automated_tests:
          command: "go test ./internal/executor/learning_integration_test.go -v"
          expected_output: "PASS"
        success_criteria:
          - "Full pipeline tested"
          - "All scenarios covered"
          - "Tests reliable and fast"
      commit:
        type: "test"
        message: "add integration tests for learning pipeline"
        body: |
          Comprehensive integration tests:
          - Full pipeline execution
          - Failure adaptation scenarios
          - Pattern detection validation
          - Run number tracking
          - Cross-run learning tests
        files:
          - "internal/executor/learning_integration_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 19
      name: "Create cross-run learning integration tests"
      agent: "test-automator"
      worktree_group: "chain-5-integration-tests"
      files:
        - "internal/executor/learning_integration_test.go"
      depends_on: [18]
      estimated_time: "1h"
      description: |
        Add tests that specifically validate learning behavior across multiple
        conductor runs, ensuring persistence and adaptation work correctly.
      test_first:
        test_file: "internal/executor/learning_integration_test.go"
        structure:
          - "TestCrossRun_LearningPersists"
          - "TestCrossRun_AgentAdaptation"
          - "TestCrossRun_RunNumberIncrement"
        mocks: []
        fixtures: []
        assertions:
          - "Data persists between runs"
          - "Second run adapts based on first"
          - "Run numbers increment correctly"
        edge_cases:
          - "Database deleted between runs"
          - "Different session IDs"
        example_skeleton: |
          func TestCrossRun_AgentAdaptation(t *testing.T) {
              dbPath := filepath.Join(t.TempDir(), "test.db")

              // Run 1: Fail twice with agent A
              run1(dbPath, "agent-a", "RED")
              run1(dbPath, "agent-a", "RED")

              // Run 2: Should adapt to agent B
              result := run2(dbPath)

              assert.NotEqual(t, "agent-a", result.AgentUsed)
          }
      implementation:
        approach: |
          1. Add cross-run test scenarios
          2. Use persistent temp database
          3. Simulate multiple conductor invocations
          4. Verify learning persists and adapts
          5. Test session and run number tracking
        code_structure: |
          func TestCrossRun_XXX(t *testing.T) {
              // Use temp file database (not :memory:)
              dbPath := filepath.Join(t.TempDir(), "test.db")

              // Run 1
              executeRun(dbPath, 1, config1)

              // Run 2 (new orchestrator, same database)
              executeRun(dbPath, 2, config2)

              // Verify cross-run behavior
              verifyLearning(dbPath)
          }
        key_points:
          - point: "Persistent database"
            details: "Use temp file not :memory: for cross-run tests"
          - point: "Session isolation"
            details: "Each run gets unique session ID"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Run cross-run tests"
            command: "go test ./internal/executor/ -run CrossRun -v"
            expected: "All cross-run tests pass"
        automated_tests:
          command: "go test ./internal/executor/learning_integration_test.go -v"
          expected_output: "PASS"
        success_criteria:
          - "Cross-run learning verified"
          - "Persistence works correctly"
      commit:
        type: "test"
        message: "add cross-run learning integration tests"
        body: |
          Test learning across multiple runs:
          - Data persistence validation
          - Agent adaptation across runs
          - Run number incrementing
          - Session management
        files:
          - "internal/executor/learning_integration_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 20
      name: "Create CLI command integration tests"
      agent: "test-automator"
      worktree_group: "chain-5-integration-tests"
      files:
        - "internal/cmd/learning_integration_test.go"
      depends_on: [19]
      estimated_time: "1h"
      description: |
        Build integration tests for all CLI learning commands to ensure they work
        correctly with the learning database end-to-end.
      test_first:
        test_file: "internal/cmd/learning_integration_test.go"
        structure:
          - "TestCLI_StatsCommand"
          - "TestCLI_ShowCommand"
          - "TestCLI_ClearCommand"
          - "TestCLI_ExportCommand"
        mocks: []
        fixtures:
          - "Test database with sample data"
        assertions:
          - "Each command produces expected output"
          - "Commands handle errors gracefully"
          - "Output formatting correct"
        edge_cases:
          - "Empty database"
          - "Missing plan file"
        example_skeleton: |
          func TestCLI_StatsCommand(t *testing.T) {
              // Setup database with data
              setupTestDatabase()

              // Execute command
              cmd := newStatsCommand()
              output := executeCommand(cmd, []string{"plan.md"})

              // Verify output
              assert.Contains(t, output, "Total executions:")
              assert.Contains(t, output, "Success rate:")
          }
      implementation:
        approach: |
          1. Create CLI integration test file
          2. Setup test database with sample data
          3. Test each command end-to-end
          4. Capture and verify output
          5. Test error scenarios
        code_structure: |
          func TestCLI_XXXCommand(t *testing.T) {
              // Setup
              dbPath := setupTestDB(t)
              seedTestData(dbPath)

              // Execute
              cmd := newXXXCommand()
              output := captureOutput(func() {
                  cmd.Execute()
              })

              // Verify
              assertOutputCorrect(t, output)
          }
        key_points:
          - point: "Output capture"
            details: "Redirect stdout to buffer for verification"
          - point: "Test data seeding"
            details: "Create realistic test scenarios"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Run CLI integration tests"
            command: "go test ./internal/cmd/learning_integration_test.go -v"
            expected: "All CLI tests pass"
        automated_tests:
          command: "go test ./internal/cmd/ -run Integration -v"
          expected_output: "PASS"
        success_criteria:
          - "All commands tested end-to-end"
          - "Output validation complete"
      commit:
        type: "test"
        message: "add CLI command integration tests"
        body: |
          Test all learning CLI commands:
          - Stats command output verification
          - Show command functionality
          - Clear command with confirmation
          - Export command (JSON/CSV)
        files:
          - "internal/cmd/learning_integration_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 21
      name: "Create comprehensive learning system documentation"
      agent: "technical-documentation-specialist"
      worktree_group: "independent-21-docs"
      files:
        - "docs/learning.md"
        - "README.md"
      depends_on: []
      estimated_time: "2h"
      description: |
        Write comprehensive documentation for the learning system including architecture,
        usage examples, configuration options, CLI commands, and troubleshooting guide.
      test_first:
        test_file: "N/A - documentation"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Create docs/learning.md with full documentation
          2. Update README.md with learning features section
          3. Include architecture diagrams (text-based)
          4. Add usage examples for all CLI commands
          5. Document configuration options
          6. Add troubleshooting section
          7. Include FAQ
        code_structure: |
          # docs/learning.md structure:

          # Adaptive Learning System

          ## Overview
          - What is it
          - How it works
          - Benefits

          ## Architecture
          - Database schema
          - Hook integration points
          - Failure analysis algorithm
          - Agent suggestion logic

          ## Configuration
          - All config options
          - Default values
          - Examples

          ## CLI Commands
          - stats
          - show
          - clear
          - export

          ## Usage Examples
          - Basic usage
          - Advanced scenarios

          ## Troubleshooting
          - Common issues
          - Solutions

          ## FAQ
        key_points:
          - point: "User-friendly"
            details: "Write for engineers unfamiliar with the system"
          - point: "Examples"
            details: "Include real command examples with output"
          - point: "Troubleshooting"
            details: "Address common issues proactively"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Review documentation"
            command: "cat docs/learning.md"
            expected: "Comprehensive and clear"
          - step: "Check links"
            command: "markdown-link-check docs/learning.md"
            expected: "All links valid"
        automated_tests:
          command: "N/A"
          expected_output: "N/A"
        success_criteria:
          - "Documentation complete"
          - "All features explained"
          - "Examples provided"
      commit:
        type: "docs"
        message: "add comprehensive learning system documentation"
        body: |
          Complete documentation for learning system:
          - docs/learning.md with full details
          - README.md updated with features
          - Architecture explanations
          - CLI command reference
          - Configuration guide
          - Troubleshooting section
          - Usage examples
        files:
          - "docs/learning.md"
          - "README.md"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 22
      name: "End-to-end validation with real plan files"
      agent: "test-automator"
      worktree_group: "chain-6-final-integration"
      files:
        - "test/integration/learning_e2e_test.go"
      depends_on: [20]
      estimated_time: "1h 30m"
      description: |
        Create end-to-end validation tests using real plan files to ensure the complete
        learning system works in production scenarios.
      test_first:
        test_file: "test/integration/learning_e2e_test.go"
        structure:
          - "TestE2E_LearningEnabled"
          - "TestE2E_LearningDisabled"
          - "TestE2E_FailureAdaptation"
        mocks: []
        fixtures:
          - "Real plan files in test/integration/fixtures/"
        assertions:
          - "Complete workflow executes"
          - "Learning adapts correctly"
          - "CLI commands work"
        edge_cases:
          - "Very large plans"
          - "Complex dependencies"
        example_skeleton: |
          func TestE2E_LearningEnabled(t *testing.T) {
              // Run conductor with real plan
              // Verify learning database created
              // Check execution recorded
              // Run again, verify adaptation
          }
      implementation:
        approach: |
          1. Create E2E test file in test/integration/
          2. Use real conductor binary
          3. Execute with real plan files
          4. Verify database created and populated
          5. Test CLI commands against real database
          6. Validate complete workflow
        code_structure: |
          func TestE2E_XXX(t *testing.T) {
              // Build conductor binary
              buildBinary(t)

              // Setup test directory
              tmpDir := t.TempDir()

              // Run conductor
              output := runConductor(tmpDir, "run", "plan.md")

              // Verify learning
              verifyLearningDatabase(tmpDir)
              verifyCLICommands(tmpDir)
          }
        key_points:
          - point: "Real binary"
            details: "Build and test actual conductor executable"
          - point: "Isolated environment"
            details: "Each test in temp directory"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Run E2E tests"
            command: "go test ./test/integration/ -run E2E -v"
            expected: "All E2E tests pass"
        automated_tests:
          command: "go test ./test/integration/learning_e2e_test.go -v"
          expected_output: "PASS"
        success_criteria:
          - "Complete system validated"
          - "Production scenarios tested"
      commit:
        type: "test"
        message: "add end-to-end validation suite"
        body: |
          Comprehensive E2E validation:
          - Real binary testing
          - Production scenarios
          - Complete workflow validation
          - CLI command testing
        files:
          - "test/integration/learning_e2e_test.go"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 23
      name: "Update example config with learning settings"
      agent: "fullstack-developer"
      worktree_group: "chain-6-final-integration"
      files:
        - ".conductor/config.yaml.example"
      depends_on: [22]
      estimated_time: "15m"
      description: |
        Update the example configuration file to include all learning settings
        with helpful comments and sensible defaults.
      test_first:
        test_file: "N/A - documentation file"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Open .conductor/config.yaml.example
          2. Add learning section with all options
          3. Include helpful comments for each setting
          4. Show default values
          5. Provide usage examples
        code_structure: |
          # Learning system configuration
          learning:
            # Enable/disable learning (default: true)
            enabled: true

            # Database path (default: .conductor/learning)
            db_path: .conductor/learning

            # Automatically adapt agent after failures (default: true)
            auto_adapt_agent: true

            # Enhance prompts with learning context (default: true)
            enhance_prompts: true

            # Minimum failures before adapting (default: 2)
            min_failures_before_adapt: 2

            # Keep execution data for N days, 0 = forever (default: 90)
            keep_executions_days: 90

            # Maximum executions to keep per task (default: 100)
            max_executions_per_task: 100
        key_points:
          - point: "Comments"
            details: "Explain each option clearly"
          - point: "Defaults shown"
            details: "Show what happens if not specified"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Review example config"
            command: "cat .conductor/config.yaml.example"
            expected: "Learning section present and clear"
        automated_tests:
          command: "N/A"
          expected_output: "N/A"
        success_criteria:
          - "All options documented"
          - "Examples helpful"
      commit:
        type: "docs"
        message: "update example config with learning settings"
        body: |
          Add learning configuration to example:
          - All learning options documented
          - Helpful comments for each setting
          - Default values shown
          - Usage guidance included
        files:
          - ".conductor/config.yaml.example"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 24
      name: "Bump version to 2.0.0 and update documentation"
      agent: "technical-documentation-specialist"
      worktree_group: "chain-6-final-integration"
      files:
        - "VERSION"
        - "CLAUDE.md"
      depends_on: [23]
      estimated_time: "30m"
      description: |
        Update VERSION file to 2.0.0 and update CLAUDE.md to document the new
        learning system architecture and features.
      test_first:
        test_file: "N/A - version bump"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Update VERSION file to 2.0.0
          2. Update CLAUDE.md with learning system info
          3. Document new architecture components
          4. Update feature list
          5. Add learning system to "Current Status"
        code_structure: |
          # VERSION file
          2.0.0

          # CLAUDE.md updates
          - Add learning system to architecture overview
          - Document new internal/learning/ package
          - Add learning configuration section
          - Update version in project overview
        key_points:
          - point: "Major version"
            details: "2.0.0 indicates significant new feature"
          - point: "Documentation completeness"
            details: "Ensure CLAUDE.md fully describes learning system"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Check version"
            command: "cat VERSION"
            expected: "2.0.0"
          - step: "Verify CLAUDE.md"
            command: "grep -i learning CLAUDE.md"
            expected: "Learning system documented"
        automated_tests:
          command: "./conductor --version"
          expected_output: "conductor version 2.0.0"
        success_criteria:
          - "Version updated to 2.0.0"
          - "Documentation complete"
      commit:
        type: "chore"
        message: "bump version to 2.0.0"
        body: |
          Release version 2.0.0 with learning system:
          - Update VERSION to 2.0.0
          - Update CLAUDE.md with learning architecture
          - Document new features and components
          - Major version for significant new capability
        files:
          - "VERSION"
          - "CLAUDE.md"
      status: "completed"
      completed_date: "2025-11-12"
    - task_number: 25
      name: "Prepare 2.0.0 release with CHANGELOG and final validation"
      agent: "fullstack-developer"
      worktree_group: "chain-6-final-integration"
      files:
        - "CHANGELOG.md"
      depends_on: [24]
      estimated_time: "45m"
      description: |
        Create comprehensive CHANGELOG for 2.0.0 release, run final validation,
        and ensure all components are production-ready.
      test_first:
        test_file: "N/A - release preparation"
        structure: []
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: ""
      implementation:
        approach: |
          1. Create/update CHANGELOG.md with 2.0.0 entry
          2. List all new features
          3. Document breaking changes (if any)
          4. Run full test suite
          5. Build binary and verify version
          6. Test all CLI commands manually
          7. Validate documentation completeness
        code_structure: |
          # CHANGELOG.md

          ## [2.0.0] - 2025-01-12

          ### Added
          - Adaptive learning system with SQLite database
          - Automatic agent adaptation after repeated failures
          - Pattern detection (compilation, test, dependency errors)
          - Session management with auto-generated IDs
          - Run number tracking per plan file
          - Four new CLI commands: stats, show, clear, export
          - Learning configuration options
          - Cross-run intelligence and prompt enhancement
          - Graceful degradation on learning failures

          ### Changed
          - Orchestrator now supports learning store integration
          - TaskExecutor includes three hook points
          - Configuration schema includes learning section

          ### Fixed
          - N/A (no fixes in this release)

          ### Breaking Changes
          - None (fully backward compatible)
        key_points:
          - point: "Comprehensive changelog"
            details: "List all features and changes"
          - point: "Validation checklist"
            details: "Verify everything works before release"
        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []
      verification:
        manual_testing:
          - step: "Run full test suite"
            command: "go test ./... -v"
            expected: "All tests pass"
          - step: "Build binary"
            command: "make build"
            expected: "Binary builds successfully"
          - step: "Check version"
            command: "./conductor --version"
            expected: "conductor version 2.0.0"
          - step: "Test learning commands"
            command: "./conductor learning --help"
            expected: "All subcommands listed"
        automated_tests:
          command: "go test ./... -race -cover"
          expected_output: "PASS, coverage >= 86%"
        success_criteria:
          - "CHANGELOG complete"
          - "All tests pass"
          - "Binary builds and runs"
          - "Documentation complete"
          - "Ready for release"
      commit:
        type: "chore"
        message: "prepare 2.0.0 release with learning system"
        body: |
          Final release preparation for v2.0.0:
          - Comprehensive CHANGELOG entry
          - All features documented
          - Full test suite passing
          - Binary validated
          - Production-ready adaptive learning system

          This major release introduces intelligent learning
          that adapts agent selection based on execution history,
          enabling conductor to improve success rates over time.
        files:
          - "CHANGELOG.md"
      status: "completed"
      completed_date: "2025-11-12"
  # Testing strategy
  testing_strategy:
    unit_tests:
      location: "internal/**/*_test.go"
      naming_convention: "*_test.go files alongside source"
      run_command: "go test ./..."
      coverage_target: "86%+"
      coverage_command: "go test ./... -cover"
    integration_tests:
      location: "internal/executor/learning_integration_test.go, internal/cmd/learning_integration_test.go"
      what_to_test:
        - "Complete learning pipeline from execution to recording"
        - "Cross-run learning scenarios"
        - "CLI command end-to-end functionality"
        - "Agent adaptation after failures"
      setup_required:
        - "In-memory SQLite database for isolated tests"
        - "Temp directories for file-based tests"
        - "Mock task execution results"
      run_command: "go test ./internal/executor/ -run Integration -v"
    e2e_tests:
      enabled: true
      location: "test/integration/learning_e2e_test.go"
      critical_flows:
        - "Full conductor execution with learning enabled"
        - "Multiple runs with failure adaptation"
        - "CLI commands against real database"
      tools: "Go test framework with real binary"
      run_command: "go test ./test/integration/ -run E2E -v"
    test_design_principles:
      patterns_to_use:
        - pattern: "Table-driven tests"
          example: |
            tests := []struct {
                name    string
                input   interface{}
                want    interface{}
                wantErr bool
            }{
                {"case 1", input1, output1, false},
                {"case 2", input2, output2, true},
            }
            for _, tt := range tests {
                t.Run(tt.name, func(t *testing.T) {
                    // test logic
                })
            }
        - pattern: "TDD Red-Green-Refactor"
          example: |
            // 1. RED: Write failing test
            // 2. GREEN: Minimal code to pass
            // 3. REFACTOR: Improve code quality
      anti_patterns_to_avoid:
        - pattern: "Testing implementation details"
          why: "Makes tests brittle and coupled to implementation"
          instead: "Test behavior and outcomes"
        - pattern: "Shared mutable state between tests"
          why: "Creates test interdependencies and flakiness"
          instead: "Use t.TempDir() and fresh databases per test"
      mocking_guidelines:
        mock_these:
          - "Claude CLI invocations (use InvokerInterface)"
          - "File system operations in some cases"
          - "Time-dependent operations"
        dont_mock_these:
          - "SQLite database (use :memory: for speed)"
          - "Business logic in learning package"
          - "Simple data transformations"
        project_mocking_pattern:
          reference: "internal/executor/task_test.go"
          example: |
            type MockInvoker struct {
                InvokeFunc func(ctx context.Context, task Task) (TaskResult, error)
            }
  # Commit strategy
  commit_strategy:
    total_commits: 25
    commits:
      - sequence: 1
        type: "feat"
        description: "Add learning database schema with SQLite"
        files: ["internal/learning/schema.sql", "internal/learning/store.go", "internal/learning/store_test.go"]
        why_separate: "Foundation - schema must exist before other components"
      - sequence: 2
        type: "feat"
        description: "Implement learning store with execution recording"
        files: ["internal/learning/store.go", "internal/learning/store_test.go"]
        why_separate: "Core CRUD operations separate from analysis logic"
      - sequence: 3
        type: "feat"
        description: "Add failure analysis and agent suggestion logic"
        files: ["internal/learning/analysis.go", "internal/learning/analysis_test.go"]
        why_separate: "Intelligence layer separate from data layer"
      - sequence: "4-7"
        type: "feat"
        description: "Executor integration (tasks 4-7)"
        files: ["internal/executor/task.go", "internal/executor/orchestrator.go", "tests"]
        why_separate: "Four commits for three hooks + orchestrator integration"
      - sequence: "8-12"
        type: "feat"
        description: "CLI commands (tasks 8-12)"
        files: ["internal/cmd/learning_*.go", "tests"]
        why_separate: "Five commits for four commands + root integration"
      - sequence: "13-17"
        type: "feat/chore"
        description: "Configuration and dependencies (tasks 13-17)"
        files: ["internal/config/", "go.mod", ".gitignore"]
        why_separate: "Independent changes, can be done in parallel"
      - sequence: "18-20"
        type: "test"
        description: "Integration tests (tasks 18-20)"
        files: ["test files"]
        why_separate: "Three commits for different integration test suites"
      - sequence: 21
        type: "docs"
        description: "Learning system documentation"
        files: ["docs/learning.md", "README.md"]
        why_separate: "Documentation as separate commit"
      - sequence: "22-25"
        type: "test/chore"
        description: "Final validation and release (tasks 22-25)"
        files: ["E2E tests", "VERSION", "CHANGELOG.md"]
        why_separate: "Four commits for E2E, config example, version, changelog"
    message_format:
      pattern: "type: brief description in present tense"
      examples:
        - "feat: add learning database schema with SQLite"
        - "feat: implement learning store with execution recording"
        - "test: add integration tests for learning pipeline"
        - "docs: add comprehensive learning system documentation"
        - "chore: bump version to 2.0.0"
      example_from_history: |
        Based on recent git history from CLAUDE.md:
        feat: add git-commit skill for clean commits
    commit_guidelines:
      - "Keep commits atomic - one task per commit"
      - "Write clear, descriptive messages in imperative mood"
      - "Follow conventional commit format (feat, fix, test, docs, chore)"
      - "Include test files in same commit as implementation"
      - "Commit after each completed task"
    worktree_commit_workflow:
      - "Each worktree maintains its own commit history on its branch"
      - "Independent worktrees can be worked on simultaneously"
      - "Chain worktrees must complete tasks sequentially"
      - "Merge worktree branches to main when complete"
      - "For tasks with cross-file dependencies, merge prerequisites first"
  # Common pitfalls
  common_pitfalls:
    - pitfall: "Forgetting CGO requirement for mattn/go-sqlite3"
      why: "mattn/go-sqlite3 requires C compiler (CGO) to build"
      how_to_avoid: "Ensure CGO_ENABLED=1 and C compiler available before building"
      reference:
        file: "go.mod"
        pattern: "CGO is required for SQLite driver"
    - pitfall: "Learning failures breaking task execution"
      why: "Learning should enhance, not break, task execution"
      how_to_avoid: "All learning operations wrapped in error handlers that log warnings but continue execution"
      reference:
        file: "internal/executor/task.go"
        pattern: "Graceful degradation pattern in hooks"
    - pitfall: "Database path resolution issues"
      why: "Relative paths can be confusing with worktrees"
      how_to_avoid: "Always resolve database path relative to plan file location"
      reference:
        file: "internal/learning/store.go"
        pattern: "Path resolution logic"
    - pitfall: "Race conditions in concurrent execution"
      why: "Multiple tasks might try to write to database simultaneously"
      how_to_avoid: "SQLite handles locking automatically, but test with -race flag"
      reference:
        command: "go test -race ./..."
        pattern: "Run race detector on all tests"
    - pitfall: "Forgetting to close database connections"
      why: "Can lead to resource leaks"
      how_to_avoid: "Always defer store.Close() after creating store"
      reference:
        file: "internal/executor/orchestrator.go"
        pattern: "defer learningStore.Close()"
    - pitfall: "Not handling :memory: vs file databases correctly in tests"
      why: "In-memory databases don't persist, file databases do"
      how_to_avoid: "Use :memory: for unit tests, temp files for integration tests"
      reference:
        file: "internal/learning/store_test.go"
        pattern: "Test database patterns"
  # Resources and references
  resources:
    existing_code:
      - type: "Executor package structure"
        path: "internal/executor/"
        note: "Study task.go, orchestrator.go for integration patterns"
      - type: "Config package patterns"
        path: "internal/config/"
        note: "Follow existing config loading and merging patterns"
      - type: "CLI command structure"
        path: "internal/cmd/"
        note: "Use cobra patterns from run.go, validate.go"
      - type: "Test patterns"
        path: "internal/executor/task_test.go"
        note: "Table-driven tests with mocks"
      - type: "Models package"
        path: "internal/models/"
        note: "Task, Plan, Result structures"
    documentation:
      - type: "SQLite Go driver"
        link: "https://github.com/mattn/go-sqlite3"
        relevance: "Database driver documentation and examples"
      - type: "Cobra CLI framework"
        link: "https://github.com/spf13/cobra"
        relevance: "CLI command structure and patterns"
      - type: "Go testing"
        link: "https://golang.org/pkg/testing/"
        relevance: "Testing patterns and best practices"
      - type: "Go database/sql"
        link: "https://golang.org/pkg/database/sql/"
        relevance: "Database operations and connection management"
      - type: "Project architecture docs"
        path: "CLAUDE.md"
        relevance: "Complete project architecture and patterns"
    external_resources:
      - title: "SQLite query optimization"
        url: "https://www.sqlite.org/queryplanner.html"
        why: "Understanding SQLite performance for learning queries"
      - title: "Go concurrency patterns"
        url: "https://go.dev/blog/pipelines"
        why: "Concurrent database access patterns"
    validation_checklist:
      - item: "All tests pass"
        command: "go test ./..."
        checked: false
      - item: "Race detector passes"
        command: "go test -race ./..."
        checked: false
      - item: "Test coverage >= 86%"
        command: "go test ./... -cover"
        checked: false
      - item: "Linter passes"
        command: "go vet ./..."
        checked: false
      - item: "Binary builds successfully"
        command: "go build ./cmd/conductor"
        checked: false
      - item: "Version displays correctly"
        command: "./conductor --version"
        checked: false
      - item: "Learning commands work"
        command: "./conductor learning --help"
        checked: false
      - item: "Documentation complete"
        verify: "docs/learning.md, README.md updated"
        checked: false
      - item: "CHANGELOG updated"
        verify: "2.0.0 entry with all features"
        checked: false
      - item: "Example config updated"
        verify: ".conductor/config.yaml.example has learning section"
        checked: false
      - item: "Worktree branches merged"
        verify: "All feature branches merged to main"
        checked: false
      - item: "Git tag created"
        command: "git tag v2.0.0"
        checked: false
