plan:
  metadata:
    feature_name: "Conductor V1 - Multi-Agent Orchestration System"
    created: "2025-11-07"
    target: "Build autonomous multi-agent orchestration system in Go for executing implementation plans with Claude Code agents"
    estimated_tasks: 25

  context:
    framework: "Go 1.21+"
    architecture: "Standard Go project layout with cobra CLI framework"
    test_framework: "Go built-in testing package"
    other_context:
      - "Uses goroutines for concurrency"
      - "Follows cobra CLI framework patterns (like kubectl, docker CLI)"
      - "Parses both Markdown and YAML plan formats"
      - "Spawns Claude Code CLI subprocesses"
      - "Implements file locking for concurrent writes"
    expectations:
      - "Write tests BEFORE implementation (TDD - Red, Green, Refactor)"
      - "Commit frequently (after each completed task)"
      - "Follow Go idioms and conventions"
      - "Keep changes minimal (YAGNI - You Aren't Gonna Need It)"
      - "Avoid duplication (DRY - Don't Repeat Yourself)"

  prerequisites:
    - item: "Go 1.21 or later installed"
      details: "Run `go version` to verify"
      verified: false
    - item: "Claude Code CLI installed and in PATH"
      details: "Run `which claude` to verify"
      verified: false
    - item: "Git initialized in project directory"
      details: "Repository should have .git directory"
      verified: true
    - item: "Editor with Go support"
      details: "VS Code + Go extension recommended"
      verified: false

  tasks:
    - task_number: 1
      name: "Initialize Go Module and Project Structure"
      files:
        - "go.mod"
        - "cmd/conductor/main.go"
        - "cmd/conductor/main_test.go"
        - "README.md"
        - ".gitignore"
      depends_on: []
      estimated_time: "15m"
      status: "completed"

      description: |
        Create the foundational Go module structure with proper directory organization following Go project layout standards.
        This establishes the base structure for the entire application.

      test_first:
        test_file: "cmd/conductor/main_test.go"

        structure:
          - "TestVersionConstant - verify main.go compiles"
          - "TestMainExists - verify main function exists"

        mocks: []

        fixtures: []

        assertions:
          - "Version constant should not be empty"
          - "Main package should compile without errors"

        edge_cases: []

        example_skeleton: |
          package main

          import (
              "testing"
          )

          func TestVersionConstant(t *testing.T) {
              if Version == "" {
                  t.Error("Version constant should not be empty")
              }
          }

      implementation:
        approach: |
          Initialize Go module, create directory structure following standard Go project layout, set up basic main.go entry point.

        code_structure: |
          conductor/
          ├── cmd/
          │   └── conductor/
          │       └── main.go          # CLI entry point
          ├── internal/
          │   ├── parser/              # Plan file parsing
          │   ├── executor/            # Task execution engine
          │   ├── agent/               # Agent discovery and invocation
          │   └── models/              # Data structures
          ├── pkg/                     # Public packages (if any)
          ├── docs/
          │   └── plans/               # Implementation plans
          ├── go.mod
          ├── go.sum
          ├── README.md
          └── Makefile

        key_points:
          - point: "Use go mod init"
            details: "Run `go mod init github.com/yourusername/conductor`"
          - point: "Version format"
            details: "const Version = \"1.0.0\""
          - point: "Follow standard layout"
            reference: "https://github.com/golang-standards/project-layout"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "No error handling needed at this stage"

      verification:
        manual_testing:
          - step: "Run go mod init"
            command: "go mod init github.com/yourusername/conductor"
            expected: "Creates go.mod file"
          - step: "Run go build"
            command: "go build ./cmd/conductor"
            expected: "Compiles without errors"
          - step: "Run the binary"
            command: "./conductor"
            expected: "Should compile without errors"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            ok      github.com/yourusername/conductor    0.001s

        success_criteria:
          - "go.mod file exists"
          - "Directory structure created"
          - "Code compiles without errors"
          - "Tests pass"

      commit:
        type: "feat"
        message: "initialize Go module and project structure"
        body: |
          - Create go.mod with Go 1.21
          - Set up standard Go project layout
          - Add basic main.go with version constant
          - Create directory structure for internal packages

        files:
          - "go.mod"
          - "cmd/conductor/main.go"
          - "README.md"
          - ".gitignore"

    - task_number: 2
      name: "Install and Configure Cobra CLI Framework"
      files:
        - "go.mod"
        - "go.sum"
        - "cmd/conductor/main.go"
        - "internal/cmd/root.go"
        - "internal/cmd/root_test.go"
      depends_on: [1]
      estimated_time: "30m"
      status: "completed"

      description: |
        Set up cobra CLI framework to handle commands like `conductor run`, `conductor validate`, with proper flag parsing.
        Cobra is the industry standard for Go CLI applications.

      test_first:
        test_file: "internal/cmd/root_test.go"

        structure:
          - "TestRootCommand - verify root command can be created"
          - "TestRootCommandHasSubcommands - verify run/validate subcommands exist"
          - "TestVersionFlag - verify --version flag works"

        mocks:
          - "cobra.Command execution"

        fixtures: []

        assertions:
          - "Root command should not be nil"
          - "Help text should contain 'Conductor'"
          - "Version flag should display version"

        edge_cases:
          - "Empty args"
          - "Invalid flags"

        example_skeleton: |
          package cmd

          import (
              "bytes"
              "testing"
          )

          func TestRootCommand(t *testing.T) {
              cmd := NewRootCommand()
              if cmd == nil {
                  t.Fatal("Root command should not be nil")
              }

              buf := new(bytes.Buffer)
              cmd.SetOut(buf)
              cmd.SetArgs([]string{"--help"})

              err := cmd.Execute()
              if err != nil {
                  t.Fatalf("Root command execution failed: %v", err)
              }

              output := buf.String()
              if !bytes.Contains([]byte(output), []byte("Conductor")) {
                  t.Error("Help text should contain 'Conductor'")
              }
          }

      implementation:
        approach: |
          Install cobra package, create root command with version flag, prepare subcommand structure.

        code_structure: |
          // internal/cmd/root.go
          package cmd

          import (
              "fmt"
              "github.com/spf13/cobra"
          )

          const Version = "1.0.0"

          func NewRootCommand() *cobra.Command {
              cmd := &cobra.Command{
                  Use:   "conductor",
                  Short: "Autonomous multi-agent orchestration system",
                  Long: `Conductor executes implementation plans by spawning and managing
          multiple Claude Code CLI agents in coordinated waves.`,
                  Version: Version,
              }

              return cmd
          }

        key_points:
          - point: "Use cobra generator or manual setup"
            details: "Manual setup preferred for simplicity"
          - point: "Add persistent flags for global options"
            details: "--verbose, --config, etc."
          - point: "Set up proper help text"
            details: "Follow cobra best practices"
          - point: "Follow cobra best practices"
            reference: "https://github.com/spf13/cobra"

        integration:
          imports:
            - "github.com/spf13/cobra"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Cobra handles command errors automatically"

      verification:
        manual_testing:
          - step: "Install cobra"
            command: "go get github.com/spf13/cobra@latest"
            expected: "Dependency added to go.mod"
          - step: "Build the binary"
            command: "go build ./cmd/conductor"
            expected: "Compiles successfully"
          - step: "Run help command"
            command: "./conductor --help"
            expected: "Displays help text with 'Conductor'"
          - step: "Run version command"
            command: "./conductor --version"
            expected: "Displays version number"

        automated_tests:
          command: "go test ./internal/cmd/..."
          expected_output: |
            Conductor v1.0.0
            Autonomous multi-agent orchestration system
            ...

        success_criteria:
          - "Cobra dependency installed"
          - "Root command works"
          - "Help text displays correctly"
          - "Tests pass"

      commit:
        type: "feat"
        message: "add cobra CLI framework"
        body: |
          - Install cobra dependency
          - Create root command with version flag
          - Set up command structure for future subcommands
          - Add comprehensive help text

        files:
          - "go.mod"
          - "go.sum"
          - "internal/cmd/root.go"
          - "internal/cmd/root_test.go"
          - "cmd/conductor/main.go"

    - task_number: 3
      name: "Define Core Data Models"
      files:
        - "internal/models/task.go"
        - "internal/models/plan.go"
        - "internal/models/result.go"
        - "internal/models/models_test.go"
      depends_on: [1]
      estimated_time: "45m"
      status: "completed"

      description: |
        Define Go structs for Plan, Task, Wave, Result, and Agent that will be used throughout the application.
        These are the core data structures that represent the domain model.

      test_first:
        test_file: "internal/models/models_test.go"

        structure:
          - "TestTaskValidation - verify task validation logic"
          - "TestDependencyCycleDetection - verify circular dependency detection"
          - "TestWaveCalculation - test dependency graph to wave conversion"

        mocks: []

        fixtures: []

        assertions:
          - "Valid task should pass validation"
          - "Invalid task should return error"
          - "Circular dependencies should be detected"

        edge_cases:
          - "Empty dependencies"
          - "Self-referencing tasks"
          - "Missing required fields"

        example_skeleton: |
          package models

          import (
              "testing"
          )

          func TestTaskValidation(t *testing.T) {
              tests := []struct {
                  name    string
                  task    Task
                  wantErr bool
              }{
                  {
                      name: "valid task",
                      task: Task{
                          Number: 1,
                          Name:   "Test Task",
                          Prompt: "Do something",
                      },
                      wantErr: false,
                  },
                  {
                      name: "missing name",
                      task: Task{
                          Number: 1,
                          Prompt: "Do something",
                      },
                      wantErr: true,
                  },
              }

              for _, tt := range tests {
                  t.Run(tt.name, func(t *testing.T) {
                      err := tt.task.Validate()
                      if (err != nil) != tt.wantErr {
                          t.Errorf("Task.Validate() error = %v, wantErr %v", err, tt.wantErr)
                      }
                  })
              }
          }

          func TestDetectCycles(t *testing.T) {
              tasks := []Task{
                  {Number: 1, DependsOn: []int{2}},
                  {Number: 2, DependsOn: []int{1}}, // Circular!
              }

              if !HasCyclicDependencies(tasks) {
                  t.Error("Should detect circular dependency")
              }
          }

      implementation:
        approach: |
          Create clean data structures with validation methods, dependency checking, and helper functions.

        code_structure: |
          // internal/models/task.go
          package models

          import (
              "errors"
              "time"
          )

          type Task struct {
              Number        int
              Name          string
              Files         []string
              DependsOn     []int
              EstimatedTime time.Duration
              Agent         string
              Prompt        string
          }

          func (t *Task) Validate() error {
              if t.Number <= 0 {
                  return errors.New("task number must be positive")
              }
              if t.Name == "" {
                  return errors.New("task name is required")
              }
              if t.Prompt == "" {
                  return errors.New("task prompt is required")
              }
              return nil
          }

          // internal/models/plan.go
          package models

          type Plan struct {
              Name          string
              Tasks         []Task
              Waves         []Wave
              DefaultAgent  string
              QualityControl QualityControlConfig
          }

          type Wave struct {
              Name           string
              TaskNumbers    []int
              MaxConcurrency int
          }

          type QualityControlConfig struct {
              Enabled      bool
              ReviewAgent  string
              RetryOnRed   int
          }

          // internal/models/result.go
          package models

          type TaskResult struct {
              Task          Task
              Status        string // "GREEN", "RED", "TIMEOUT", "FAILED"
              Output        string
              Error         error
              Duration      time.Duration
              RetryCount    int
              ReviewFeedback string
          }

          type ExecutionResult struct {
              TotalTasks    int
              Completed     int
              Failed        int
              Duration      time.Duration
              FailedTasks   []TaskResult
          }

        key_points:
          - point: "Use time.Duration for time fields"
            details: "Standard library type for durations"
          - point: "Implement Validate() methods"
            details: "Validation logic on each struct"
          - point: "Add helper functions for common operations"
            details: "Like HasCyclicDependencies()"
          - point: "Use exported fields for JSON/YAML marshaling"
            details: "All fields start with capital letter"

        integration:
          imports:
            - "time"
            - "errors"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return descriptive errors from Validate() methods"

      verification:
        manual_testing:
          - step: "Create sample instances of each struct"
            command: "In Go code, instantiate Task, Plan, etc."
            expected: "Should compile"
          - step: "Call Validate() methods"
            command: "task.Validate()"
            expected: "Should return nil for valid, error for invalid"
          - step: "Verify errors are returned correctly"
            command: "Check error messages"
            expected: "Clear, descriptive error messages"

        automated_tests:
          command: "go test ./internal/models/..."
          expected_output: |
            ok      github.com/yourusername/conductor/internal/models    0.002s

        success_criteria:
          - "All struct types defined"
          - "Validation methods implemented"
          - "Tests pass"
          - "No compilation errors"

      commit:
        type: "feat"
        message: "define core data models"
        body: |
          - Add Task struct with validation
          - Add Plan and Wave structs
          - Add Result structs for execution tracking
          - Implement dependency cycle detection helpers

        files:
          - "internal/models/task.go"
          - "internal/models/plan.go"
          - "internal/models/result.go"
          - "internal/models/models_test.go"

    - task_number: 4
      name: "Implement Markdown Plan Parser"
      files:
        - "internal/parser/markdown.go"
        - "internal/parser/markdown_test.go"
        - "internal/parser/testdata/sample-plan.md"
        - "go.mod"
        - "go.sum"
      depends_on: [3]
      estimated_time: "2h"
      status: "completed"

      description: |
        Parse Markdown files generated by `/doc` command, extracting tasks with metadata, dependencies, and optional conductor frontmatter.
        This is a critical component for reading implementation plans.

      test_first:
        test_file: "internal/parser/markdown_test.go"

        structure:
          - "TestParseMarkdownPlan - parse valid markdown"
          - "TestExtractTasks - extract task sections"
          - "TestParseFrontmatter - parse YAML frontmatter"
          - "TestParseTaskMetadata - extract File(s), Depends on, Estimated time"
          - "TestParseTaskPrompt - extract full task content as prompt"

        mocks: []

        fixtures:
          - "testdata/sample-plan.md"

        assertions:
          - "Task count matches expected"
          - "Dependencies parsed correctly"
          - "Prompt extracted from full task content"
          - "Metadata fields extracted correctly"

        edge_cases:
          - "No frontmatter"
          - "Missing metadata fields"
          - "Empty task sections"

        example_skeleton: |
          package parser

          import (
              "strings"
              "testing"

              "github.com/yourusername/conductor/internal/models"
          )

          func TestParseMarkdownPlan(t *testing.T) {
              markdown := `# Implementation Plan: Test Plan

          **Created**: 2025-11-07
          **Estimated Tasks**: 2

          ## Task 1: First Task

          **File(s)**: ` + "`file1.go`" + `
          **Depends on**: None
          **Estimated time**: 30m

          ### What you're building
          Test task description

          ### Implementation
          Implementation details here
          `

              parser := NewMarkdownParser()
              plan, err := parser.Parse(strings.NewReader(markdown))
              if err != nil {
                  t.Fatalf("Failed to parse markdown: %v", err)
              }

              if len(plan.Tasks) != 1 {
                  t.Errorf("Expected 1 task, got %d", len(plan.Tasks))
              }

              task := plan.Tasks[0]
              if task.Number != 1 {
                  t.Errorf("Expected task number 1, got %d", task.Number)
              }
              if task.Name != "First Task" {
                  t.Errorf("Expected task name 'First Task', got '%s'", task.Name)
              }
          }

      implementation:
        approach: |
          Use goldmark markdown parser library to parse markdown, extract task sections based on `## Task N:` headings,
          parse metadata fields with regex, combine full task section as prompt.

        code_structure: |
          // internal/parser/markdown.go
          package parser

          import (
              "bufio"
              "fmt"
              "io"
              "regexp"
              "strconv"
              "strings"
              "time"

              "github.com/yuin/goldmark"
              "github.com/yuin/goldmark/ast"
              "github.com/yuin/goldmark/text"
              "gopkg.in/yaml.v3"

              "github.com/yourusername/conductor/internal/models"
          )

          type MarkdownParser struct {
              markdown goldmark.Markdown
          }

          func NewMarkdownParser() *MarkdownParser {
              return &MarkdownParser{
                  markdown: goldmark.New(),
              }
          }

          func (p *MarkdownParser) Parse(r io.Reader) (*models.Plan, error) {
              // Read full content
              content, err := io.ReadAll(r)
              if err != nil {
                  return nil, err
              }

              // Extract frontmatter if present
              plan := &models.Plan{}
              content, frontmatter := extractFrontmatter(content)
              if frontmatter != nil {
                  if err := parseConductorConfig(frontmatter, plan); err != nil {
                      return nil, err
                  }
              }

              // Parse markdown AST
              doc := p.markdown.Parser().Parse(text.NewReader(content))

              // Extract tasks
              tasks, err := p.extractTasks(doc, content)
              if err != nil {
                  return nil, err
              }

              plan.Tasks = tasks
              return plan, nil
          }

        key_points:
          - point: "Use goldmark for robust markdown parsing"
            details: "goldmark is the standard Go markdown parser"
          - point: "Regex for metadata extraction"
            details: "Fields like **File(s)**: are parsed with regex"
          - point: "Full task section becomes the prompt"
            details: "Everything from ## Task N: to next ## heading"
          - point: "Handle optional YAML frontmatter"
            details: "Conductor config can be in frontmatter"

        integration:
          imports:
            - "github.com/yuin/goldmark"
            - "gopkg.in/yaml.v3"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return error if markdown parsing fails"
            - "Return error if task extraction fails"

      verification:
        manual_testing:
          - step: "Create test markdown file"
            command: "Create testdata/sample-plan.md with /doc output"
            expected: "File created"
          - step: "Run parser on file"
            command: "parser.ParseFile(path)"
            expected: "Tasks extracted"
          - step: "Print extracted tasks and verify"
            command: "Print task details"
            expected: "All fields populated correctly"

        automated_tests:
          command: "go test ./internal/parser/ -v"
          expected_output: |
            === RUN   TestParseMarkdownPlan
            --- PASS: TestParseMarkdownPlan (0.00s)
            === RUN   TestExtractTasks
            --- PASS: TestExtractTasks (0.00s)
            PASS

        success_criteria:
          - "Markdown parsing works"
          - "Tasks extracted correctly"
          - "Metadata parsed"
          - "Tests pass with 80%+ coverage"

      commit:
        type: "feat"
        message: "implement markdown plan parser"
        body: |
          - Add goldmark dependency for markdown parsing
          - Extract tasks from ## Task N: headings
          - Parse metadata fields (File(s), Depends on, Estimated time, Agent)
          - Extract full task content as prompt
          - Support optional YAML frontmatter for conductor config

        files:
          - "internal/parser/markdown.go"
          - "internal/parser/markdown_test.go"
          - "internal/parser/testdata/sample-plan.md"
          - "go.mod"
          - "go.sum"

    - task_number: 5
      name: "Implement YAML Plan Parser"
      files:
        - "internal/parser/yaml.go"
        - "internal/parser/yaml_test.go"
        - "internal/parser/testdata/sample-plan.yaml"
      depends_on: [3]
      estimated_time: "1h"
      status: "completed"
      completed_date: "2025-11-07"
      git_commits:
        - "c933989 (initial implementation)"
        - "5b6586b (enhanced for real YAML schema)"
      qa_status: "GREEN"
      test_coverage: "63.8%"
      notes: "Successfully handles real conductor-v1-implementation.yaml with complex nested structures. All 8 test functions passing. Supports both simple and complex YAML schemas."

      description: |
        Parse YAML files generated by `/doc-yaml` command, extracting structured task definitions with full metadata.
        This provides an alternative to Markdown format with more structure.

      test_first:
        test_file: "internal/parser/yaml_test.go"

        structure:
          - "TestParseYAMLPlan - parse valid YAML"
          - "TestExtractYAMLTasks - extract tasks array"
          - "TestParseDependencies - parse depends_on field"
          - "TestParseConductorConfig - parse optional conductor section"

        mocks: []

        fixtures:
          - "testdata/sample-plan.yaml"

        assertions:
          - "Tasks array extracted"
          - "Dependencies parsed correctly"
          - "Conductor config parsed"

        edge_cases:
          - "Missing optional fields"
          - "Empty arrays"
          - "Nested YAML structures"

        example_skeleton: |
          package parser

          import (
              "strings"
              "testing"
          )

          func TestParseYAMLPlan(t *testing.T) {
              yaml := `
          plan:
            metadata:
              feature_name: "Test Plan"
              estimated_tasks: 2
            tasks:
              - task_number: 1
                name: "First Task"
                estimated_time: "30m"
                depends_on: []
                description: "Test description"
          `

              parser := NewYAMLParser()
              plan, err := parser.Parse(strings.NewReader(yaml))
              if err != nil {
                  t.Fatalf("Failed to parse YAML: %v", err)
              }

              if len(plan.Tasks) != 1 {
                  t.Errorf("Expected 1 task, got %d", len(plan.Tasks))
              }

              task := plan.Tasks[0]
              if task.Number != 1 {
                  t.Errorf("Expected task number 1, got %d", task.Number)
              }
          }

      implementation:
        approach: |
          Use gopkg.in/yaml.v3 to unmarshal YAML structure, map YAML schema to models.Plan/Task structs,
          build prompt from description + implementation + test_first sections.

        code_structure: |
          // internal/parser/yaml.go
          package parser

          import (
              "fmt"
              "io"
              "time"

              "gopkg.in/yaml.v3"

              "github.com/yourusername/conductor/internal/models"
          )

          type YAMLParser struct{}

          type yamlPlan struct {
              Conductor *conductorConfig `yaml:"conductor"`
              Plan      struct {
                  Metadata struct {
                      FeatureName    string `yaml:"feature_name"`
                      EstimatedTasks int    `yaml:"estimated_tasks"`
                  } `yaml:"metadata"`
                  Tasks []yamlTask `yaml:"tasks"`
              } `yaml:"plan"`
          }

          type yamlTask struct {
              TaskNumber    int      `yaml:"task_number"`
              Name          string   `yaml:"name"`
              Files         []string `yaml:"files"`
              DependsOn     []int    `yaml:"depends_on"`
              EstimatedTime string   `yaml:"estimated_time"`
              Description   string   `yaml:"description"`
              TestFirst     struct {
                  TestFile string `yaml:"test_file"`
                  Example  string `yaml:"example_skeleton"`
              } `yaml:"test_first"`
              Implementation struct {
                  Approach string `yaml:"approach"`
                  Code     string `yaml:"code_structure"`
              } `yaml:"implementation"`
          }

          func NewYAMLParser() *YAMLParser {
              return &YAMLParser{}
          }

          func (p *YAMLParser) Parse(r io.Reader) (*models.Plan, error) {
              var yp yamlPlan
              decoder := yaml.NewDecoder(r)
              if err := decoder.Decode(&yp); err != nil {
                  return nil, fmt.Errorf("failed to decode YAML: %w", err)
              }

              plan := &models.Plan{
                  Name: yp.Plan.Metadata.FeatureName,
              }

              // Parse conductor config if present
              if yp.Conductor != nil {
                  parseConductorConfigYAML(yp.Conductor, plan)
              }

              // Convert YAML tasks to models.Task
              for _, yt := range yp.Plan.Tasks {
                  task := models.Task{
                      Number:    yt.TaskNumber,
                      Name:      yt.Name,
                      Files:     yt.Files,
                      DependsOn: yt.DependsOn,
                  }

                  // Parse estimated time
                  if dur, err := parseTimeString(yt.EstimatedTime); err == nil {
                      task.EstimatedTime = dur
                  }

                  // Build comprehensive prompt from all sections
                  task.Prompt = buildPromptFromYAML(&yt)

                  plan.Tasks = append(plan.Tasks, task)
              }

              return plan, nil
          }

        key_points:
          - point: "Define YAML schema structs"
            details: "Matching /doc-yaml output"
          - point: "Use struct tags for YAML field mapping"
            details: "`yaml:\"field_name\"`"
          - point: "Combine multiple YAML sections into prompt"
            details: "description + test_first + implementation"
          - point: "Handle missing optional fields gracefully"
            details: "Check for nil before accessing"

        integration:
          imports:
            - "gopkg.in/yaml.v3"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return error if YAML unmarshaling fails"
            - "Handle missing required fields"

      verification:
        manual_testing:
          - step: "Create test YAML file"
            command: "Create testdata/sample-plan.yaml"
            expected: "File created"
          - step: "Run parser on file"
            command: "parser.Parse(reader)"
            expected: "Plan parsed"
          - step: "Verify all fields extracted correctly"
            command: "Check task fields"
            expected: "All fields populated"

        automated_tests:
          command: "go test ./internal/parser/ -run TestYAML -v"
          expected_output: |
            === RUN   TestParseYAMLPlan
            --- PASS: TestParseYAMLPlan (0.00s)
            PASS

        success_criteria:
          - "YAML parsing works"
          - "All fields extracted"
          - "Prompt built correctly"
          - "Tests pass"

      commit:
        type: "feat"
        message: "implement YAML plan parser"
        body: |
          - Parse /doc-yaml format with structured schema
          - Extract tasks from plan.tasks array
          - Build comprehensive prompts from description/test/implementation sections
          - Support optional conductor configuration block

        files:
          - "internal/parser/yaml.go"
          - "internal/parser/yaml_test.go"
          - "internal/parser/testdata/sample-plan.yaml"

    - task_number: 6
      name: "Implement Plan Parser Interface and Auto-Detection"
      files:
        - "internal/parser/parser.go"
        - "internal/parser/parser_test.go"
      depends_on: [4, 5]
      estimated_time: "30m"
      actual_time: "30m"
      status: "completed"
      completed_date: "2025-11-08"
      git_commit: "5d6bd14"
      qa_status: "GREEN"
      test_coverage: "68.2%"
      tests_passing: "25/25"
      quality_score: "95/100"

      description: |
        Unified Parser interface that auto-detects plan format (Markdown vs YAML) based on file extension or content, returning parsed models.Plan.

      test_first:
        test_file: "internal/parser/parser_test.go"

        structure:
          - "TestAutoDetectMarkdown - verify .md extension triggers markdown parser"
          - "TestAutoDetectYAML - verify .yaml/.yml extension triggers YAML parser"
          - "TestParseFromFile - integration test for file reading and parsing"

        mocks: []

        fixtures:
          - "testdata/ files"

        assertions:
          - "Correct parser selected for .md files"
          - "Correct parser selected for .yaml files"
          - "ParseFile returns valid plan"

        edge_cases:
          - "Unknown file extensions"
          - "Missing files"

        example_skeleton: |
          package parser

          import (
              "os"
              "path/filepath"
              "testing"
          )

          func TestAutoDetectFormat(t *testing.T) {
              tests := []struct {
                  filename string
                  want     Format
              }{
                  {"plan.md", FormatMarkdown},
                  {"plan.yaml", FormatYAML},
                  {"plan.yml", FormatYAML},
                  {"unknown.txt", FormatUnknown},
              }

              for _, tt := range tests {
                  t.Run(tt.filename, func(t *testing.T) {
                      got := DetectFormat(tt.filename)
                      if got != tt.want {
                          t.Errorf("DetectFormat(%s) = %v, want %v", tt.filename, got, tt.want)
                      }
                  })
              }
          }

          func TestParseFromFile(t *testing.T) {
              mdPath := filepath.Join("testdata", "sample-plan.md")
              plan, err := ParseFile(mdPath)
              if err != nil {
                  t.Fatalf("ParseFile failed: %v", err)
              }

              if len(plan.Tasks) == 0 {
                  t.Error("Expected tasks to be parsed")
              }
          }

      implementation:
        approach: |
          Create Parser interface, implement auto-detection based on file extension, provide convenient ParseFile() function.

        code_structure: |
          // internal/parser/parser.go
          package parser

          import (
              "fmt"
              "io"
              "os"
              "path/filepath"
              "strings"

              "github.com/yourusername/conductor/internal/models"
          )

          type Format int

          const (
              FormatUnknown Format = iota
              FormatMarkdown
              FormatYAML
          )

          type Parser interface {
              Parse(r io.Reader) (*models.Plan, error)
          }

          func DetectFormat(filename string) Format {
              ext := strings.ToLower(filepath.Ext(filename))
              switch ext {
              case ".md", ".markdown":
                  return FormatMarkdown
              case ".yaml", ".yml":
                  return FormatYAML
              default:
                  return FormatUnknown
              }
          }

          func NewParser(format Format) (Parser, error) {
              switch format {
              case FormatMarkdown:
                  return NewMarkdownParser(), nil
              case FormatYAML:
                  return NewYAMLParser(), nil
              default:
                  return nil, fmt.Errorf("unsupported format: %v", format)
              }
          }

          func ParseFile(path string) (*models.Plan, error) {
              format := DetectFormat(path)
              if format == FormatUnknown {
                  return nil, fmt.Errorf("unknown file format: %s", path)
              }

              parser, err := NewParser(format)
              if err != nil {
                  return nil, err
              }

              file, err := os.Open(path)
              if err != nil {
                  return nil, fmt.Errorf("failed to open file: %w", err)
              }
              defer file.Close()

              plan, err := parser.Parse(file)
              if err != nil {
                  return nil, fmt.Errorf("failed to parse plan: %w", err)
              }

              // Set plan file path for later updates
              plan.FilePath = path

              return plan, nil
          }

        key_points:
          - point: "Use interface for polymorphism"
            details: "Parser interface allows different implementations"
          - point: "Auto-detection makes CLI usage simple"
            details: "Users don't need to specify format"
          - point: "Store original file path in plan"
            details: "Needed for updating task status later"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return error for unknown formats"
            - "Return error if file doesn't exist"

      verification:
        manual_testing:
          - step: "Call ParseFile with .md file"
            expected: "Markdown parser used, tasks extracted"
          - step: "Call ParseFile with .yaml file"
            expected: "YAML parser used, tasks extracted"

        automated_tests:
          command: "go test ./internal/parser/ -v"
          expected_output: |
            PASS
            ok      github.com/yourusername/conductor/internal/parser    0.003s

        success_criteria:
          - "Auto-detection works"
          - "Both parsers accessible via interface"
          - "Tests pass"

      commit:
        type: "feat"
        message: "add unified parser interface with auto-detection"
        body: |
          - Create Parser interface for both formats
          - Implement auto-detection based on file extension
          - Add convenient ParseFile() function
          - Store file path in plan for later updates

        files:
          - "internal/parser/parser.go"
          - "internal/parser/parser_test.go"

    - task_number: 7
      name: "Implement Dependency Graph and Wave Calculator"
      files:
        - "internal/executor/graph.go"
        - "internal/executor/graph_test.go"
      depends_on: [3]
      estimated_time: "1.5h"
      actual_time: "1.5h"
      status: "completed"
      completed_at: "2025-11-08"
      git_commit: "3b2db76"
      test_coverage: "94.4%"
      tests_passing: "31/31"
      qa_status: "GREEN"

      description: |
        Build dependency graph from tasks, detect cycles, calculate execution waves using topological sort (Kahn's algorithm).

      test_first:
        test_file: "internal/executor/graph_test.go"

        structure:
          - "TestBuildGraph - verify graph construction from tasks"
          - "TestDetectCycle - detect circular dependencies"
          - "TestCalculateWaves - calculate execution waves from DAG"
          - "TestTopologicalSort - verify Kahn's algorithm implementation"
          - "TestIndependentTasks - tasks with no deps go in Wave 1"

        mocks: []

        fixtures: []

        assertions:
          - "Graph built correctly from tasks"
          - "Cycles detected"
          - "Waves calculated correctly"
          - "Independent tasks in first wave"

        edge_cases:
          - "No dependencies"
          - "Complex dependency chains"
          - "Self-referencing tasks"

        example_skeleton: |
          package executor

          import (
              "testing"

              "github.com/yourusername/conductor/internal/models"
          )

          func TestDetectCycle(t *testing.T) {
              tests := []struct {
                  name      string
                  tasks     []models.Task
                  wantCycle bool
              }{
                  {
                      name: "no cycle",
                      tasks: []models.Task{
                          {Number: 1, DependsOn: []int{}},
                          {Number: 2, DependsOn: []int{1}},
                      },
                      wantCycle: false,
                  },
                  {
                      name: "simple cycle",
                      tasks: []models.Task{
                          {Number: 1, DependsOn: []int{2}},
                          {Number: 2, DependsOn: []int{1}},
                      },
                      wantCycle: true,
                  },
                  {
                      name: "self reference",
                      tasks: []models.Task{
                          {Number: 1, DependsOn: []int{1}},
                      },
                      wantCycle: true,
                  },
              }

              for _, tt := range tests {
                  t.Run(tt.name, func(t *testing.T) {
                      graph := BuildDependencyGraph(tt.tasks)
                      hasCycle := graph.HasCycle()
                      if hasCycle != tt.wantCycle {
                          t.Errorf("HasCycle() = %v, want %v", hasCycle, tt.wantCycle)
                      }
                  })
              }
          }

          func TestCalculateWaves(t *testing.T) {
              tasks := []models.Task{
                  {Number: 1, Name: "Task 1", DependsOn: []int{}},
                  {Number: 2, Name: "Task 2", DependsOn: []int{1}},
                  {Number: 3, Name: "Task 3", DependsOn: []int{1}},
                  {Number: 4, Name: "Task 4", DependsOn: []int{2, 3}},
              }

              waves, err := CalculateWaves(tasks)
              if err != nil {
                  t.Fatalf("CalculateWaves failed: %v", err)
              }

              // Expected: Wave 1: [1], Wave 2: [2,3], Wave 3: [4]
              if len(waves) != 3 {
                  t.Errorf("Expected 3 waves, got %d", len(waves))
              }

              if len(waves[0].TaskNumbers) != 1 || waves[0].TaskNumbers[0] != 1 {
                  t.Errorf("Wave 1 should contain task 1")
              }

              if len(waves[1].TaskNumbers) != 2 {
                  t.Errorf("Wave 2 should contain 2 tasks")
              }
          }

      implementation:
        approach: |
          Build adjacency list representation of task dependencies, implement DFS for cycle detection, use Kahn's algorithm for topological sort and wave grouping.

        code_structure: |
          // internal/executor/graph.go
          package executor

          import (
              "fmt"

              "github.com/yourusername/conductor/internal/models"
          )

          type DependencyGraph struct {
              Tasks    map[int]*models.Task
              Edges    map[int][]int // task -> dependencies
              InDegree map[int]int   // task -> number of dependencies
          }

          func BuildDependencyGraph(tasks []models.Task) *DependencyGraph {
              g := &DependencyGraph{
                  Tasks:    make(map[int]*models.Task),
                  Edges:    make(map[int][]int),
                  InDegree: make(map[int]int),
              }

              // Build task map and initialize in-degree
              for i := range tasks {
                  g.Tasks[tasks[i].Number] = &tasks[i]
                  g.InDegree[tasks[i].Number] = 0
              }

              // Build edges and calculate in-degree
              for _, task := range tasks {
                  for _, dep := range task.DependsOn {
                      g.Edges[dep] = append(g.Edges[dep], task.Number)
                      g.InDegree[task.Number]++
                  }
              }

              return g
          }

          func (g *DependencyGraph) HasCycle() bool {
              // Use DFS with color marking
              white := 0 // not visited
              gray := 1  // visiting
              black := 2 // visited

              colors := make(map[int]int)
              for taskNum := range g.Tasks {
                  colors[taskNum] = white
              }

              var dfs func(int) bool
              dfs = func(node int) bool {
                  colors[node] = gray

                  for _, neighbor := range g.Edges[node] {
                      if colors[neighbor] == gray {
                          return true // back edge = cycle
                      }
                      if colors[neighbor] == white && dfs(neighbor) {
                          return true
                      }
                  }

                  colors[node] = black
                  return false
              }

              for taskNum := range g.Tasks {
                  if colors[taskNum] == white {
                      if dfs(taskNum) {
                          return true
                      }
                  }
              }

              return false
          }

          func CalculateWaves(tasks []models.Task) ([]models.Wave, error) {
              graph := BuildDependencyGraph(tasks)

              // Check for cycles first
              if graph.HasCycle() {
                  return nil, fmt.Errorf("circular dependency detected")
              }

              // Kahn's algorithm for topological sort + wave grouping
              var waves []models.Wave
              inDegree := make(map[int]int)
              for k, v := range graph.InDegree {
                  inDegree[k] = v
              }

              for len(inDegree) > 0 {
                  // Find all tasks with in-degree 0 (current wave)
                  var currentWave []int
                  for taskNum, degree := range inDegree {
                      if degree == 0 {
                          currentWave = append(currentWave, taskNum)
                      }
                  }

                  if len(currentWave) == 0 {
                      return nil, fmt.Errorf("graph error: no tasks with zero in-degree")
                  }

                  // Create wave
                  wave := models.Wave{
                      Name:           fmt.Sprintf("Wave %d", len(waves)+1),
                      TaskNumbers:    currentWave,
                      MaxConcurrency: 10, // default
                  }
                  waves = append(waves, wave)

                  // Remove current wave tasks and update in-degrees
                  for _, taskNum := range currentWave {
                      delete(inDegree, taskNum)

                      // Decrease in-degree for dependent tasks
                      for _, dependent := range graph.Edges[taskNum] {
                          if _, exists := inDegree[dependent]; exists {
                              inDegree[dependent]--
                          }
                      }
                  }
              }

              return waves, nil
          }

        key_points:
          - point: "Use DFS with color marking for cycle detection"
            details: "White-gray-black algorithm"
          - point: "Kahn's algorithm groups independent tasks"
            details: "Tasks with zero in-degree can run in parallel"
          - point: "Tasks with zero dependencies go in Wave 1"
            details: "These can start immediately"
          - point: "Each wave's tasks can run in parallel"
            details: "Within a wave, no dependencies exist"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling:
            - "Return error if circular dependency detected"
            - "Return error if graph is invalid"

      verification:
        manual_testing:
          - step: "Create sample task lists with dependencies"
            expected: "Graph built correctly"
          - step: "Call CalculateWaves"
            expected: "Waves calculated correctly"

        automated_tests:
          command: "go test ./internal/executor/ -run TestGraph -v"
          expected_output: |
            === RUN   TestDetectCycle
            --- PASS: TestDetectCycle (0.00s)
            === RUN   TestCalculateWaves
            --- PASS: TestCalculateWaves (0.00s)
            PASS

        success_criteria:
          - "Cycle detection works"
          - "Wave calculation correct"
          - "Tests pass"

      commit:
        type: "feat"
        message: "implement dependency graph and wave calculator"
        body: |
          - Build adjacency list dependency graph from tasks
          - Implement DFS-based cycle detection
          - Implement Kahn's algorithm for topological sort
          - Calculate execution waves grouping independent tasks

        files:
          - "internal/executor/graph.go"
          - "internal/executor/graph_test.go"

    - task_number: 8
      name: "Implement Agent Discovery"
      files:
        - "internal/agent/discovery.go"
        - "internal/agent/discovery_test.go"
        - "internal/agent/testdata/sample-agent.md"
      depends_on: [3]
      estimated_time: "45m"
      status: "completed"
      completed: "2025-11-08"
      git_commit: "231ac7a"
      qa_status: "YELLOW (91.3% test coverage, 10/10 tests passing, 1 spec clarification needed)"
      actual_time: "45m"

      description: |
        Scan ~/.claude/agents/ directory for available agents, parse agent metadata files, provide agent lookup by name.

      test_first:
        test_file: "internal/agent/discovery_test.go"

        structure:
          - "TestScanAgentsDirectory - verify directory scanning"
          - "TestParseAgentFile - parse .md agent definition"
          - "TestAgentExists - check if agent name exists"
          - "TestFallbackToGeneralPurpose - verify fallback logic"

        mocks: []

        fixtures:
          - "testdata/agents/"

        assertions:
          - "Agents discovered from directory"
          - "Agent metadata parsed correctly"
          - "Agent lookup works"

        edge_cases:
          - "Missing agents directory"
          - "Invalid agent files"
          - "Empty directory"

        example_skeleton: |
          package agent

          import (
              "os"
              "path/filepath"
              "testing"
          )

          func TestDiscoverAgents(t *testing.T) {
              // Create temp directory with test agent files
              tmpDir := t.TempDir()

              // Write sample agent file
              agentContent := `---
          name: test-agent
          description: Test agent
          tools: Read, Write
          ---
          Test agent prompt
          `
              err := os.WriteFile(filepath.Join(tmpDir, "test-agent.md"), []byte(agentContent), 0644)
              if err != nil {
                  t.Fatal(err)
              }

              registry := NewRegistry(tmpDir)
              agents, err := registry.Discover()
              if err != nil {
                  t.Fatalf("Discover failed: %v", err)
              }

              if len(agents) != 1 {
                  t.Errorf("Expected 1 agent, got %d", len(agents))
              }

              if _, exists := agents["test-agent"]; !exists {
                  t.Error("Expected test-agent to exist")
              }
          }

          func TestAgentExists(t *testing.T) {
              registry := NewRegistry("testdata/agents")
              registry.Discover()

              if !registry.Exists("swiftdev") {
                  t.Error("swiftdev agent should exist")
              }

              if registry.Exists("nonexistent-agent") {
                  t.Error("nonexistent-agent should not exist")
              }
          }

      implementation:
        approach: |
          Walk ~/.claude/agents/ directory, parse .md files with YAML frontmatter, extract agent name and metadata, store in registry map.

        code_structure: |
          // internal/agent/discovery.go
          package agent

          import (
              "fmt"
              "os"
              "path/filepath"
              "strings"

              "gopkg.in/yaml.v3"
          )

          type Agent struct {
              Name        string
              Description string
              Tools       []string
              FilePath    string
          }

          type Registry struct {
              AgentsDir string
              agents    map[string]*Agent
          }

          func NewRegistry(agentsDir string) *Registry {
              if agentsDir == "" {
                  // Default to ~/.claude/agents
                  home, _ := os.UserHomeDir()
                  agentsDir = filepath.Join(home, ".claude", "agents")
              }

              return &Registry{
                  AgentsDir: agentsDir,
                  agents:    make(map[string]*Agent),
              }
          }

          func (r *Registry) Discover() (map[string]*Agent, error) {
              // Check if directory exists
              if _, err := os.Stat(r.AgentsDir); os.IsNotExist(err) {
                  // No agents directory - return empty map, not an error
                  return r.agents, nil
              }

              // Walk directory
              err := filepath.Walk(r.AgentsDir, func(path string, info os.FileInfo, err error) error {
                  if err != nil {
                      return err
                  }

                  if info.IsDir() {
                      return nil
                  }

                  // Only process .md files
                  if !strings.HasSuffix(path, ".md") {
                      return nil
                  }

                  agent, err := parseAgentFile(path)
                  if err != nil {
                      // Log warning but continue
                      fmt.Fprintf(os.Stderr, "Warning: failed to parse %s: %v\n", path, err)
                      return nil
                  }

                  r.agents[agent.Name] = agent
                  return nil
              })

              return r.agents, err
          }

          func (r *Registry) Exists(agentName string) bool {
              _, exists := r.agents[agentName]
              return exists
          }

          func (r *Registry) Get(agentName string) (*Agent, bool) {
              agent, exists := r.agents[agentName]
              return agent, exists
          }

        key_points:
          - point: "Handle missing agents directory gracefully"
            details: "Return empty map, not error"
          - point: "Parse YAML frontmatter from .md files"
            details: "Agent metadata in frontmatter"
          - point: "Store agents in map for fast lookup"
            details: "O(1) lookup by name"
          - point: "Default to ~/.claude/agents if not specified"
            details: "Standard location"

        integration:
          imports:
            - "gopkg.in/yaml.v3"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Log warnings for unparseable files"
            - "Continue on individual file errors"

      verification:
        manual_testing:
          - step: "Point to ~/.claude/agents directory"
            expected: "Agents discovered"
          - step: "Call Discover"
            expected: "All agents found and parsed"

        automated_tests:
          command: "go test ./internal/agent/ -v"
          expected_output: |
            === RUN   TestDiscoverAgents
            --- PASS: TestDiscoverAgents (0.00s)
            PASS

        success_criteria:
          - "Directory scanning works"
          - "Agent parsing works"
          - "Lookup works"
          - "Tests pass"

      commit:
        type: "feat"
        message: "implement agent discovery"
        body: |
          - Scan ~/.claude/agents/ directory for agent definitions
          - Parse agent metadata from YAML frontmatter
          - Store agents in registry map for lookup
          - Handle missing directory gracefully

        files:
          - "internal/agent/discovery.go"
          - "internal/agent/discovery_test.go"
          - "internal/agent/testdata/sample-agent.md"

    - task_number: 9
      name: "Implement Claude CLI Agent Invocation"
      files:
        - "internal/agent/invoker.go"
        - "internal/agent/invoker_test.go"
      depends_on: [3, 8]
      estimated_time: "1h"
      status: "pending"

      description: |
        Build and execute claude CLI commands with proper flags (--settings, -p, --output-format json), capture output, handle timeouts.

      test_first:
        test_file: "internal/agent/invoker_test.go"

        structure:
          - "TestBuildCommand - verify command construction"
          - "TestInvokeAgent - test agent invocation (mock claude CLI)"
          - "TestTimeout - verify timeout handling"
          - "TestOutputCapture - verify stdout/stderr capture"

        mocks:
          - "exec.Command (for testing)"

        fixtures: []

        assertions:
          - "Command built with correct flags"
          - "Timeout handled correctly"
          - "Output captured"

        edge_cases:
          - "Command timeout"
          - "Non-zero exit code"
          - "Missing claude binary"

        example_skeleton: |
          package agent

          import (
              "context"
              "testing"
              "time"

              "github.com/yourusername/conductor/internal/models"
          )

          func TestBuildCommand(t *testing.T) {
              task := models.Task{
                  Number: 1,
                  Name:   "Test Task",
                  Prompt: "Do something",
                  Agent:  "swiftdev",
                  EstimatedTime: 30 * time.Minute,
              }

              invoker := NewInvoker()
              args := invoker.BuildCommandArgs(task)

              // Verify required flags
              hasP := false
              hasSettings := false

              for i, arg := range args {
                  if arg == "-p" {
                      hasP = true
                  }
                  if arg == "--settings" && i+1 < len(args) {
                      if strings.Contains(args[i+1], "disableAllHooks") {
                          hasSettings = true
                      }
                  }
              }

              if !hasP {
                  t.Error("Command should have -p flag")
              }
              if !hasSettings {
                  t.Error("Command should have --settings with disableAllHooks")
              }
          }

      implementation:
        approach: |
          Use os/exec to spawn claude CLI subprocess, construct args with required flags, use context.WithTimeout for timeout handling, capture stdout/stderr.

        code_structure: |
          // internal/agent/invoker.go
          package agent

          import (
              "context"
              "encoding/json"
              "fmt"
              "os/exec"
              "strings"
              "time"

              "github.com/yourusername/conductor/internal/models"
          )

          type Invoker struct {
              ClaudePath string
              Registry   *Registry
          }

          type InvocationResult struct {
              Output   string
              ExitCode int
              Duration time.Duration
              Error    error
          }

          func NewInvoker() *Invoker {
              return &Invoker{
                  ClaudePath: "claude", // assume in PATH
              }
          }

          func (inv *Invoker) Invoke(ctx context.Context, task models.Task) (*InvocationResult, error) {
              startTime := time.Now()

              // Build command args
              args := inv.BuildCommandArgs(task)

              // Create command with context (for timeout)
              cmd := exec.CommandContext(ctx, inv.ClaudePath, args...)

              // Capture output
              output, err := cmd.CombinedOutput()

              result := &InvocationResult{
                  Output:   string(output),
                  Duration: time.Since(startTime),
              }

              if err != nil {
                  if exitErr, ok := err.(*exec.ExitError); ok {
                      result.ExitCode = exitErr.ExitCode()
                  } else {
                      result.Error = err
                  }
              }

              return result, nil
          }

          func (inv *Invoker) BuildCommandArgs(task models.Task) []string {
              args := []string{
                  "-p", // Print mode (non-interactive)
              }

              // Build prompt with agent reference if specified
              prompt := task.Prompt
              if task.Agent != "" && inv.Registry != nil && inv.Registry.Exists(task.Agent) {
                  // Reference agent in prompt
                  prompt = fmt.Sprintf("use the %s subagent to: %s", task.Agent, task.Prompt)
              }

              args = append(args, prompt)

              // Disable hooks for automation
              args = append(args, "--settings", `{"disableAllHooks": true}`)

              // JSON output for easier parsing
              args = append(args, "--output-format", "json")

              return args
          }

          func (inv *Invoker) InvokeWithTimeout(task models.Task, timeout time.Duration) (*InvocationResult, error) {
              ctx, cancel := context.WithTimeout(context.Background(), timeout)
              defer cancel()

              return inv.Invoke(ctx, task)
          }

        key_points:
          - point: "Always use -p flag for non-interactive mode"
            details: "Required for automation"
          - point: "Always use --settings with disableAllHooks"
            details: "Prevents interactive prompts"
          - point: "Use context for timeout propagation"
            details: "Graceful timeout handling"
          - point: "Capture both stdout and stderr"
            details: "Get all output"
          - point: "Parse JSON output if available"
            details: "Structured output format"

        integration:
          imports:
            - "os/exec"
            - "context"
          services_to_inject: []
          config_values: []
          error_handling:
            - "Handle command timeout"
            - "Handle non-zero exit codes"
            - "Handle missing binary"

      verification:
        manual_testing:
          - step: "Create test task"
            expected: "Task created"
          - step: "Call Invoke (requires claude in PATH)"
            expected: "Command executed"

        automated_tests:
          command: "go test ./internal/agent/ -run TestBuild -v"
          expected_output: |
            === RUN   TestBuildCommand
            --- PASS: TestBuildCommand (0.00s)
            PASS

        success_criteria:
          - "Command building works"
          - "Flags correct"
          - "Tests pass"

      commit:
        type: "feat"
        message: "implement Claude CLI agent invocation"
        body: |
          - Build claude CLI commands with required flags
          - Add context-based timeout handling
          - Capture stdout/stderr output
          - Parse JSON output format
          - Support agent-specific prompt construction

        files:
          - "internal/agent/invoker.go"
          - "internal/agent/invoker_test.go"

    - task_number: 10
      name: "Implement Quality Control Review Agent"
      files:
        - "internal/executor/review.go"
        - "internal/executor/review_test.go"
      depends_on: [9]
      estimated_time: "1h"
      status: "pending"

      description: |
        Build review prompts with task output, parse GREEN/RED responses, retry logic with attempt tracking.
        Quality control ensures tasks are completed correctly before proceeding.

      test_first:
        test_file: "internal/executor/review_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Build review prompts that include task output, invoke review agent, parse GREEN/RED status from response,
          implement retry logic for RED responses up to configured max attempts.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Build review prompts with task output"
            details: "Include original task and agent output"
          - point: "Parse GREEN/RED responses"
            details: "Extract status from review agent output"
          - point: "Retry logic with attempt tracking"
            details: "Retry RED tasks up to max attempts"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Review prompts built correctly"
          - "GREEN/RED parsing works"
          - "Retry logic implemented"

      commit:
        type: "feat"
        message: "implement quality control review agent"
        body: ""
        files:
          - "internal/executor/review.go"
          - "internal/executor/review_test.go"

    - task_number: 11
      name: "Implement File Locking for Plan Updates"
      files:
        - "internal/updater/lock.go"
        - "internal/updater/lock_test.go"
      depends_on: [3]
      estimated_time: "45m"
      status: "pending"

      description: |
        Use github.com/gofrs/flock for file locking, atomic file writes with temp file + rename,
        update checkboxes or YAML status fields.

      test_first:
        test_file: "internal/updater/lock_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Use flock library for cross-process file locking, write to temp file first,
          then atomically rename to target file, ensuring concurrent updates are safe.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Use github.com/gofrs/flock"
            details: "Cross-process file locking"
          - point: "Atomic file writes with temp file + rename"
            details: "Prevents partial writes"
          - point: "Update checkboxes or YAML status fields"
            details: "Modify plan file safely"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "File locking works"
          - "Atomic writes implemented"
          - "Concurrent updates safe"

      commit:
        type: "feat"
        message: "implement file locking for plan updates"
        body: ""
        files:
          - "internal/updater/lock.go"
          - "internal/updater/lock_test.go"

    - task_number: 12
      name: "Implement Plan Updater"
      files:
        - "internal/updater/updater.go"
        - "internal/updater/updater_test.go"
      depends_on: [11]
      estimated_time: "1h"
      status: "pending"

      description: |
        Find and update task checkboxes in Markdown, update YAML fields (status, completed_at),
        handle concurrent updates safely.

      test_first:
        test_file: "internal/updater/updater_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Parse plan file, find task entries, update status fields (Markdown checkboxes or YAML status),
          write back with file locking to prevent conflicts.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Find and update task checkboxes in Markdown"
            details: "Regex-based checkbox updating"
          - point: "Update YAML fields (status, completed_at)"
            details: "Modify YAML structure"
          - point: "Handle concurrent updates safely"
            details: "Use file locking"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Markdown checkbox updates work"
          - "YAML updates work"
          - "Concurrent updates safe"

      commit:
        type: "feat"
        message: "implement plan updater"
        body: ""
        files:
          - "internal/updater/updater.go"
          - "internal/updater/updater_test.go"

    - task_number: 13
      name: "Implement Wave Executor"
      files:
        - "internal/executor/wave.go"
        - "internal/executor/wave_test.go"
      depends_on: [7, 14]
      estimated_time: "2h"
      status: "pending"

      description: |
        Execute waves sequentially, spawn goroutines for parallel tasks within wave,
        bounded concurrency with semaphore pattern, collect results via channels.

      test_first:
        test_file: "internal/executor/wave_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          For each wave, spawn goroutines for tasks (up to max concurrency), use semaphore/channel
          to limit concurrent execution, collect results via result channel, wait for wave completion
          before starting next wave.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Execute waves sequentially"
            details: "Wait for wave to complete before next"
          - point: "Spawn goroutines for parallel tasks within wave"
            details: "Concurrent execution within wave"
          - point: "Bounded concurrency with semaphore pattern"
            details: "Limit max parallel tasks"
          - point: "Collect results via channels"
            details: "Goroutine-safe result collection"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Wave execution sequential"
          - "Tasks execute in parallel within wave"
          - "Concurrency bounded correctly"

      commit:
        type: "feat"
        message: "implement wave executor"
        body: ""
        files:
          - "internal/executor/wave.go"
          - "internal/executor/wave_test.go"

    - task_number: 14
      name: "Implement Task Executor"
      files:
        - "internal/executor/task.go"
        - "internal/executor/task_test.go"
      depends_on: [9, 10, 12]
      estimated_time: "1.5h"
      status: "pending"

      description: |
        Execute single task: invoke → review → retry, handle RED flags with retry logic,
        update plan file on completion, return TaskResult.

      test_first:
        test_file: "internal/executor/task_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Invoke agent for task, if quality control enabled invoke review agent, if RED and retries available
          retry task, update plan file with status, return TaskResult with all metadata.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Execute single task: invoke → review → retry"
            details: "Full task execution pipeline"
          - point: "Handle RED flags with retry logic"
            details: "Retry on quality control failure"
          - point: "Update plan file on completion"
            details: "Mark task complete in plan"
          - point: "Return TaskResult"
            details: "Complete result metadata"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Task execution pipeline works"
          - "Retry logic works"
          - "Plan updates on completion"

      commit:
        type: "feat"
        message: "implement task executor"
        body: ""
        files:
          - "internal/executor/task.go"
          - "internal/executor/task_test.go"

    - task_number: 15
      name: "Implement Main Orchestration Engine"
      files:
        - "internal/executor/orchestrator.go"
        - "internal/executor/orchestrator_test.go"
      depends_on: [13]
      estimated_time: "2h"
      status: "pending"

      description: |
        Coordinate wave executor, handle graceful shutdown (SIGINT), collect and aggregate results,
        print final summary.

      test_first:
        test_file: "internal/executor/orchestrator_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Calculate waves from plan, execute waves sequentially via wave executor, handle SIGINT for graceful shutdown,
          collect all task results, generate execution summary, return ExecutionResult.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Coordinate wave executor"
            details: "Top-level execution coordination"
          - point: "Handle graceful shutdown (SIGINT)"
            details: "Cancel context on signal"
          - point: "Collect and aggregate results"
            details: "Gather all task results"
          - point: "Print final summary"
            details: "Success/failure counts, duration, etc."

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Orchestration coordinates waves"
          - "Graceful shutdown works"
          - "Summary printed correctly"

      commit:
        type: "feat"
        message: "implement main orchestration engine"
        body: ""
        files:
          - "internal/executor/orchestrator.go"
          - "internal/executor/orchestrator_test.go"

    - task_number: 16
      name: "Implement `conductor run` Command"
      files:
        - "internal/cmd/run.go"
        - "internal/cmd/run_test.go"
      depends_on: [15]
      estimated_time: "1h"
      status: "pending"

      description: |
        Parse flags (--dry-run, --max-concurrency, etc.), load plan file, call orchestration engine,
        display progress.

      test_first:
        test_file: "internal/cmd/run_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create cobra command for 'run', parse CLI flags, load plan file with parser,
          create orchestrator with config, execute and display progress.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Parse flags (--dry-run, --max-concurrency, etc.)"
            details: "CLI flag handling"
          - point: "Load plan file"
            details: "Use parser to load plan"
          - point: "Call orchestration engine"
            details: "Execute the plan"
          - point: "Display progress"
            details: "Real-time progress updates"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Run command works"
          - "Flags parsed correctly"
          - "Progress displayed"

      commit:
        type: "feat"
        message: "implement conductor run command"
        body: ""
        files:
          - "internal/cmd/run.go"
          - "internal/cmd/run_test.go"

    - task_number: 17
      name: "Implement `conductor validate` Command"
      files:
        - "internal/cmd/validate.go"
        - "internal/cmd/validate_test.go"
      depends_on: [6, 7]
      estimated_time: "45m"
      status: "pending"

      description: |
        Parse plan file, run validation checks, display validation report, return appropriate exit code.

      test_first:
        test_file: "internal/cmd/validate_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create cobra command for 'validate', load plan file, run validations (task validation, cycle detection),
          display report, exit with appropriate code.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Parse plan file"
            details: "Load and parse plan"
          - point: "Run validation checks"
            details: "Task validation, cycle detection"
          - point: "Display validation report"
            details: "User-friendly output"
          - point: "Return appropriate exit code"
            details: "0 for valid, 1 for invalid"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Validate command works"
          - "All validations run"
          - "Report displayed correctly"

      commit:
        type: "feat"
        message: "implement conductor validate command"
        body: ""
        files:
          - "internal/cmd/validate.go"
          - "internal/cmd/validate_test.go"

    - task_number: 18
      name: "Implement Console Logger"
      files:
        - "internal/logger/console.go"
        - "internal/logger/console_test.go"
      depends_on: [3]
      estimated_time: "1h"
      status: "pending"

      description: |
        Timestamp-prefixed log messages, task start/complete/fail messages,
        wave progress tracking, color coding (optional).

      test_first:
        test_file: "internal/logger/console_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create console logger with timestamp formatting, methods for different log levels,
          task lifecycle logging, wave progress indicators.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Timestamp-prefixed log messages"
            details: "All logs have timestamps"
          - point: "Task start/complete/fail messages"
            details: "Lifecycle logging"
          - point: "Wave progress tracking"
            details: "Show wave completion"
          - point: "Color coding (optional)"
            details: "Use terminal colors if supported"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Console logging works"
          - "Timestamps added"
          - "Progress tracking visible"

      commit:
        type: "feat"
        message: "implement console logger"
        body: ""
        files:
          - "internal/logger/console.go"
          - "internal/logger/console_test.go"

    - task_number: 19
      name: "Implement File Logger"
      files:
        - "internal/logger/file.go"
        - "internal/logger/file_test.go"
      depends_on: [18]
      estimated_time: "45m"
      status: "pending"

      description: |
        Create .conductor/logs/ directory, write per-run log files, write per-task detailed logs,
        create latest.log symlink.

      test_first:
        test_file: "internal/logger/file_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create log directory if not exists, generate timestamped log file for run,
          write task outputs to individual files, symlink latest run log.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Create .conductor/logs/ directory"
            details: "Ensure directory exists"
          - point: "Write per-run log files"
            details: "Timestamped log files"
          - point: "Write per-task detailed logs"
            details: "Individual task logs"
          - point: "Create latest.log symlink"
            details: "Always points to latest run"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "File logging works"
          - "Log directory created"
          - "Symlink created correctly"

      commit:
        type: "feat"
        message: "implement file logger"
        body: ""
        files:
          - "internal/logger/file.go"
          - "internal/logger/file_test.go"

    - task_number: 20
      name: "Add Configuration File Support"
      files:
        - "internal/config/config.go"
        - "internal/config/config_test.go"
      depends_on: [2]
      estimated_time: "1h"
      status: "pending"

      description: |
        Parse .conductor/config.yaml, merge config with CLI flags, default values.

      test_first:
        test_file: "internal/config/config_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Load .conductor/config.yaml if exists, parse YAML config, merge with CLI flags
          (CLI flags take precedence), provide default values for missing config.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Parse .conductor/config.yaml"
            details: "Load config from file"
          - point: "Merge config with CLI flags"
            details: "CLI flags override config file"
          - point: "Default values"
            details: "Provide sensible defaults"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Config file parsing works"
          - "Merging works correctly"
          - "Defaults applied"

      commit:
        type: "feat"
        message: "add configuration file support"
        body: ""
        files:
          - "internal/config/config.go"
          - "internal/config/config_test.go"

    - task_number: 21
      name: "Add Error Handling and Recovery"
      files:
        - "internal/executor/errors.go"
        - "internal/executor/errors_test.go"
      depends_on: [15]
      estimated_time: "1h"
      status: "pending"

      description: |
        Graceful error handling throughout, continue-on-error strategy, timeout handling,
        resource cleanup.

      test_first:
        test_file: "internal/executor/errors_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Add error wrapping throughout, implement continue-on-error for task failures,
          proper context timeout handling, defer cleanup for resources.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Graceful error handling throughout"
            details: "Proper error wrapping and logging"
          - point: "Continue-on-error strategy"
            details: "Don't stop on single task failure"
          - point: "Timeout handling"
            details: "Handle context timeouts gracefully"
          - point: "Resource cleanup"
            details: "Defer cleanup, close channels"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Error handling robust"
          - "Continue-on-error works"
          - "Resources cleaned up"

      commit:
        type: "feat"
        message: "add error handling and recovery"
        body: ""
        files:
          - "internal/executor/errors.go"
          - "internal/executor/errors_test.go"

    - task_number: 22
      name: "Add Integration Tests"
      files:
        - "test/integration/orchestrator_test.go"
        - "test/integration/fixtures/"
      depends_on: [16, 17]
      estimated_time: "2h"
      status: "pending"

      description: |
        End-to-end test with sample plan, test both Markdown and YAML formats,
        test failure scenarios, test --dry-run mode.

      test_first:
        test_file: "test/integration/orchestrator_test.go"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create integration test suite with sample plans, test full execution flow,
          test both formats, test error scenarios, test dry-run mode.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "End-to-end test with sample plan"
            details: "Full workflow test"
          - point: "Test both Markdown and YAML formats"
            details: "Format compatibility"
          - point: "Test failure scenarios"
            details: "Error handling verification"
          - point: "Test --dry-run mode"
            details: "Dry-run functionality"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Integration tests pass"
          - "Both formats tested"
          - "Error scenarios covered"

      commit:
        type: "test"
        message: "add integration tests"
        body: ""
        files:
          - "test/integration/orchestrator_test.go"
          - "test/integration/fixtures/"

    - task_number: 23
      name: "Add Makefile and Build Script"
      files:
        - "Makefile"
        - "scripts/build.sh"
      depends_on: [16]
      estimated_time: "30m"
      status: "pending"

      description: |
        make build - compile binary, make test - run all tests, make install - install to PATH,
        cross-compilation targets.

      test_first:
        test_file: "N/A"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Create Makefile with common targets, build script for cross-compilation,
          install target to copy binary to PATH.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "make build - compile binary"
            details: "Build conductor binary"
          - point: "make test - run all tests"
            details: "Run full test suite"
          - point: "make install - install to PATH"
            details: "Copy to /usr/local/bin or similar"
          - point: "Cross-compilation targets"
            details: "Build for multiple platforms"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Makefile works"
          - "Build targets work"
          - "Install works"

      commit:
        type: "chore"
        message: "add Makefile and build script"
        body: ""
        files:
          - "Makefile"
          - "scripts/build.sh"

    - task_number: 24
      name: "Write README and Documentation"
      files:
        - "README.md"
        - "docs/usage.md"
        - "docs/plan-format.md"
      depends_on: [23]
      estimated_time: "1h"
      status: "pending"

      description: |
        Installation instructions, usage examples, plan format documentation, troubleshooting guide.

      test_first:
        test_file: "N/A"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Write comprehensive README with installation, usage, and examples.
          Create separate docs for plan format and troubleshooting.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Installation instructions"
            details: "How to install and setup"
          - point: "Usage examples"
            details: "Common workflows"
          - point: "Plan format documentation"
            details: "Markdown and YAML format specs"
          - point: "Troubleshooting guide"
            details: "Common issues and solutions"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "README complete"
          - "Usage docs clear"
          - "Format documented"

      commit:
        type: "docs"
        message: "write README and documentation"
        body: ""
        files:
          - "README.md"
          - "docs/usage.md"
          - "docs/plan-format.md"

    - task_number: 25
      name: "Final Integration and Testing"
      files:
        - "Various files"
      depends_on: [22, 23, 24]
      estimated_time: "2h"
      status: "pending"

      description: |
        Run full orchestration with real plans, test error scenarios, performance testing,
        bug fixes and polish.

      test_first:
        test_file: "N/A"
        structure:
          - "Tests to be defined during implementation"
        mocks: []
        fixtures: []
        assertions: []
        edge_cases: []
        example_skeleton: |
          // To be defined during implementation

      implementation:
        approach: |
          Run conductor on real implementation plans, test various scenarios,
          identify and fix bugs, performance optimization if needed.

        code_structure: |
          // To be defined during implementation

        key_points:
          - point: "Run full orchestration with real plans"
            details: "End-to-end testing with real data"
          - point: "Test error scenarios"
            details: "Verify error handling"
          - point: "Performance testing"
            details: "Check performance characteristics"
          - point: "Bug fixes and polish"
            details: "Final cleanup and fixes"

        integration:
          imports: []
          services_to_inject: []
          config_values: []
          error_handling: []

      verification:
        manual_testing:
          - step: "Manual testing steps from summary"
            expected: "Expected outcome"

        automated_tests:
          command: "go test ./..."
          expected_output: |
            Tests passing

        success_criteria:
          - "Full orchestration works"
          - "All scenarios tested"
          - "Performance acceptable"
          - "No critical bugs"

      commit:
        type: "chore"
        message: "final integration and testing"
        body: ""
        files:
          - "Various files"

  testing_strategy:
    unit_tests:
      location: "*_test.go files alongside implementation"
      naming_convention: "TestFunctionName for functions, TestTypeName_MethodName for methods"
      run_command: "go test ./..."
      coverage_target: "70%"
      coverage_command: "go test -cover ./..."

    integration_tests:
      location: "test/integration/"
      what_to_test:
        - "Full orchestration flows with sample plans"
        - "End-to-end plan parsing and execution"
        - "Multi-wave task execution"
      setup_required:
        - "Sample plan files"
        - "Mock agent responses"
        - "Test fixtures"
      run_command: "go test ./test/integration/..."

    e2e_tests:
      enabled: false
      location: "N/A"
      critical_flows: []
      tools: "N/A"
      run_command: "N/A"

    test_design_principles:
      patterns_to_use:
        - pattern: "Table-driven tests"
          example: |
            tests := []struct {
                name    string
                input   Task
                wantErr bool
            }{
                {"valid task", Task{...}, false},
                {"invalid task", Task{}, true},
            }
            for _, tt := range tests {
                t.Run(tt.name, func(t *testing.T) {
                    err := tt.input.Validate()
                    if (err != nil) != tt.wantErr {
                        t.Errorf("got error = %v, wantErr %v", err, tt.wantErr)
                    }
                })
            }

        - pattern: "Subtests with t.Run()"
          example: |
            func TestFeature(t *testing.T) {
                t.Run("scenario 1", func(t *testing.T) { ... })
                t.Run("scenario 2", func(t *testing.T) { ... })
            }

        - pattern: "Test fixtures in testdata/"
          example: |
            testdata/
            ├── sample-plan.md
            ├── sample-plan.yaml
            └── invalid-plan.md

      anti_patterns_to_avoid:
        - pattern: "Testing implementation details"
          why: "Makes tests brittle and coupled to implementation"
          instead: "Test behavior and outcomes, not internals"

        - pattern: "Tests with external dependencies"
          why: "Makes tests slow and unreliable"
          instead: "Use mocks and test doubles"

        - pattern: "Brittle tests that break on refactoring"
          why: "Creates maintenance burden"
          instead: "Test public interfaces, not private functions"

      mocking_guidelines:
        mock_these:
          - "External claude CLI (use test scripts)"
          - "File system operations (when appropriate)"
          - "Network calls"

        dont_mock_these:
          - "Internal functions"
          - "Simple utilities"
          - "Data structures"

        project_mocking_pattern:
          reference: "Use interfaces for testability"
          example: |
            // Define interface
            type Parser interface {
                Parse(r io.Reader) (*Plan, error)
            }

            // Mock implementation for tests
            type MockParser struct {
                ParseFunc func(r io.Reader) (*Plan, error)
            }

  commit_strategy:
    total_commits: 25

    commits:
      - sequence: 1
        type: "feat"
        description: "initialize Go module and project structure"
        files:
          - "go.mod"
          - "cmd/conductor/main.go"
          - "README.md"
          - ".gitignore"
        why_separate: "Foundation must be established first"

      - sequence: 2
        type: "feat"
        description: "add cobra CLI framework"
        files:
          - "go.mod"
          - "go.sum"
          - "internal/cmd/root.go"
          - "internal/cmd/root_test.go"
          - "cmd/conductor/main.go"
        why_separate: "CLI framework is independent infrastructure"

      - sequence: 3
        type: "feat"
        description: "define core data models"
        files:
          - "internal/models/task.go"
          - "internal/models/plan.go"
          - "internal/models/result.go"
          - "internal/models/models_test.go"
        why_separate: "Core domain models are foundation for other features"

      - sequence: 4
        type: "feat"
        description: "implement markdown plan parser"
        files:
          - "internal/parser/markdown.go"
          - "internal/parser/markdown_test.go"
          - "internal/parser/testdata/sample-plan.md"
          - "go.mod"
          - "go.sum"
        why_separate: "Parser is substantial feature, deserves own commit"

      - sequence: 5
        type: "feat"
        description: "implement YAML plan parser"
        files:
          - "internal/parser/yaml.go"
          - "internal/parser/yaml_test.go"
          - "internal/parser/testdata/sample-plan.yaml"
        why_separate: "Alternative parser format, separate concern"

    message_format:
      pattern: "type: brief description in present tense"
      examples:
        - "feat: add user authentication with JWT"
        - "fix: resolve race condition in async handler"
        - "test: add edge case coverage for validation"
        - "refactor: extract common logic to utility"

      example_from_history: |
        feat: implement markdown plan parser

    commit_guidelines:
      - "Keep commits atomic - one logical change per commit"
      - "Write clear, descriptive messages in imperative mood"
      - "Commit after each completed task"
      - "Tests and implementation in same commit (after TDD red-green)"

  common_pitfalls:
    - pitfall: "Goroutine leaks"
      why: "Forgetting to close channels or wait for goroutines"
      how_to_avoid: "Use sync.WaitGroup, defer close(), context cancellation"
      reference:
        file: "Standard Go concurrency patterns"
        pattern: "Always clean up goroutines"

    - pitfall: "Race conditions in plan updates"
      why: "Multiple goroutines writing to plan file"
      how_to_avoid: "Use flock file locking"
      reference:
        file: "github.com/gofrs/flock examples"
        pattern: "Lock before write, unlock after"

    - pitfall: "Timeout not propagating"
      why: "Not passing context through call chain"
      how_to_avoid: "Always pass context.Context as first parameter"
      reference:
        file: "Go context package documentation"
        pattern: "Context should be first parameter"

    - pitfall: "Circular dependencies not detected"
      why: "Incorrect graph traversal"
      how_to_avoid: "Implement proper DFS with color marking"
      reference:
        file: "Task 7 graph implementation"
        pattern: "Use white-gray-black coloring for cycle detection"

  resources:
    existing_code:
      - type: "Completed tasks"
        path: "internal/models/"
        note: "Study the models package structure"

      - type: "Test examples"
        path: "internal/parser/markdown_test.go"
        note: "Follow testing patterns used here"

      - type: "Go idioms"
        path: "All existing .go files"
        note: "Maintain consistent code style"

    documentation:
      - type: "Go Project Layout"
        link: "https://github.com/golang-standards/project-layout"
        relevance: "Standard project structure"

      - type: "Cobra CLI"
        link: "https://github.com/spf13/cobra"
        relevance: "CLI framework documentation"

      - type: "Effective Go"
        link: "https://go.dev/doc/effective_go"
        relevance: "Go best practices"

    external_resources:
      - title: "Goldmark documentation"
        url: "https://github.com/yuin/goldmark"
        why: "Markdown parsing library"

      - title: "YAML v3 for Go"
        url: "https://github.com/go-yaml/yaml"
        why: "YAML parsing library"

      - title: "File locking in Go"
        url: "https://github.com/gofrs/flock"
        why: "Concurrent file access"

    validation_checklist:
      - item: "All tests pass"
        command: "go test ./..."
        checked: false

      - item: "Code formatted correctly"
        command: "gofmt -w ."
        checked: false

      - item: "Linter passes"
        command: "golangci-lint run"
        checked: false

      - item: "No race conditions"
        command: "go test -race ./..."
        checked: false

      - item: "Build succeeds"
        command: "go build ./cmd/conductor"
        checked: false

      - item: "Binary works"
        command: "./conductor --help"
        checked: false

      - item: "Documentation updated"
        files: "README.md, relevant docs"
        checked: false
