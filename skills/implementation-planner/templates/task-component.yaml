# Component Task Template
# For single-module/feature implementation
# Updated: v2.3.0 - Added data flow and anti-pattern criteria

- task_number: N
  name: "Descriptive Task Name"
  agent: "agent-name"  # From agent discovery
  worktree_group: "chain-1"  # Or "independent-N"
  files:
    - "path/to/file1.ext"
    - "path/to/file2.ext"
  depends_on: []  # Or [task_numbers]
  estimated_time: "30m"  # 5m, 15m, 30m, 1h, 2h

  # ─────────────────────────────────────────────────────────────
  # DATA FLOW DOCUMENTATION (Conductor ignores, humans see)
  # ─────────────────────────────────────────────────────────────
  # For tasks WITH dependencies, document what functions/types
  # are consumed from upstream tasks:
  #
  # calls:
  #   - function: "FunctionName"
  #     from_task: N
  #     package: "path/to/package"
  #   - function: "OtherFunction"
  #     from_task: M
  #     package: "path/to/other"
  #
  # produces:  # What this task creates for downstream
  #   - function: "NewFunction"
  #     file: "path/to/file.ext"
  #   - type: "NewType"
  #     file: "path/to/file.ext"
  # ─────────────────────────────────────────────────────────────

  # Task-level fields for conductor QC parsing
  success_criteria:
    # Functional criteria (task-specific)
    - "Specific measurable outcome"
    - "Verifiable condition"
    - "Testable requirement"

    # Data integration criteria (if task has depends_on)
    # - "Calls FunctionX() from Task N with real data"
    # - "Uses TypeY from Task M correctly"

    # Anti-pattern criteria (AUTO-APPEND to ALL tasks)
    - "No TODO comments in production code paths"
    - "No placeholder empty structs (e.g., Type{})"
    - "No unused variables (_ = x pattern)"
    - "All imports from dependency tasks resolve"

  test_commands:
    - "language-specific test command"
    - "go test ./... -v"
    - "npm test"
    - "pytest tests/ -v"

  # ─────────────────────────────────────────────────────────────
  # DESCRIPTION WITH DEPENDENCY VERIFICATION
  # ─────────────────────────────────────────────────────────────
  # For tasks WITH dependencies, include Phase 0 verification.
  # For foundation tasks (depends_on: []), skip Phase 0.
  description: |
    # Task N: [Task Name]

    ## PHASE 0: DEPENDENCY VERIFICATION (EXECUTE FIRST)

    **Verify dependencies before implementing:**

    ```bash
    # Verify Task X complete (FunctionName exists)
    grep -q "func FunctionName" path/to/file.ext && \
      echo "✓ Task X: FunctionName ready" || \
      echo "❌ STOP: Task X incomplete"

    # Build check
    go build ./path/to/... && echo "✓ Compiles" || echo "❌ Build fails"
    ```

    **If ANY check fails:** Report "Dependency Task X incomplete" and STOP.

    ---

    ## IMPLEMENTATION REQUIREMENTS

    **You MUST call these functions (from upstream tasks):**
    - `package.FunctionName()` - from Task X

    **PROHIBITED:**
    - ❌ TODO comments in production code
    - ❌ Placeholder empty structs
    - ❌ Unused variables
    - ❌ Mocking upstream functions instead of calling them

    ---

    ## TASK DESCRIPTION

    2-3 sentences explaining WHAT and WHY.
    Clear context for engineer.

  test_first:
    test_file: "path/to/test_file.ext"

    structure:
      - "describe/test block for feature"
      - "test case 1: specific behavior"
      - "test case 2: edge case"
      - "test case 3: error condition"

    mocks:
      - "dependency1"
      - "dependency2"

    fixtures:
      - "fixture1"

    assertions:
      - "outcome1"
      - "outcome2"

    edge_cases:
      - "edge case 1"
      - "edge case 2"

    example_skeleton: |
      # See quality-gates.md for complete test patterns

  implementation:
    approach: |
      Detailed explanation of implementation approach.
      Architectural decisions and reasoning.

    code_structure: |
      // Pseudocode or skeleton showing structure
      class FeatureName {
        method() {
          // Logic
        }
      }

    key_points:
      - point: "Follow pattern from X"
        reference: "path/to/similar.ext:line"
      - point: "Use utility Y"
        reference: "utilityName from path"

    integration:
      imports:
        - "import statement or package"
      services_to_inject:
        - "Service from path"
      config_values:
        - "CONFIG_VAR from config"
      error_handling:
        - "Approach to errors"

  verification:
    automated_tests:
      command: "test command"
      expected_output: "All tests passing"

    success_criteria:  # Duplicates task-level for docs
      - "All tests pass"
      - "Type checker passes"
      - "Code compiles"

    # Anti-pattern verification
    anti_pattern_checks:
      - command: "grep -n 'TODO' path/to/file.ext || true"
        expect: "no matches in production paths"
      - command: "grep -n '_ =' path/to/file.ext || true"
        expect: "no unused variable assignments"

  code_quality:  # REQUIRED
    # Include language-specific pipeline
    python:
      full_quality_pipeline:
        command: |
          python -m black . && \
          python -m mypy src/ && \
          python -m pytest
        description: "Complete quality pipeline"
        exit_on_failure: true

    typescript:
      full_quality_pipeline:
        command: |
          npx prettier --write . && \
          npx tsc --noEmit && \
          npm test
        description: "Complete quality pipeline"
        exit_on_failure: true

    go:
      full_quality_pipeline:
        command: |
          gofmt -w . && \
          go vet ./... && \
          go test ./...
        description: "Complete quality pipeline"
        exit_on_failure: true

  commit:
    type: "feat"  # feat, fix, test, refactor, docs, chore
    message: "descriptive commit message"
    body: "Optional explanation"
    files:
      - "path/to/file1.ext"
      - "path/to/file2.ext"
